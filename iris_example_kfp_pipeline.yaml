# PIPELINE DEFINITION
# Name: iris-example
# Description: Generated from annotated notebook
components:
  comp-data-loading:
    executorLabel: exec-data-loading
    outputDefinitions:
      artifacts:
        artifacts:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
  comp-evaluation:
    executorLabel: exec-evaluation
    outputDefinitions:
      artifacts:
        metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
  comp-pipeline-metrics-9:
    executorLabel: exec-pipeline-metrics-9
    outputDefinitions:
      parameters:
        metrics:
          parameterType: STRUCT
  comp-pipeline-parameters-8:
    executorLabel: exec-pipeline-parameters-8
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRUCT
  comp-preprocessing:
    executorLabel: exec-preprocessing
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRING
  comp-training:
    executorLabel: exec-training
    outputDefinitions:
      artifacts:
        metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
deploymentSpec:
  executors:
    exec-data-loading:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - data_loading
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'matplotlib'\
          \ 'numpy' 'pandas' 'scikit-learn' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef data_loading() -> NamedTuple('Outputs', [('dataset', Dataset),\
          \ ('metrics', Metrics), ('artifacts', Artifact)]):\n    \"\"\"\n    Pipeline\
          \ step: step:data-loading\n    Generated from notebook cell 4\n    Original\
          \ tags: step:data-loading\n    \"\"\"\n    # Imports\n    import os\n  \
          \  import sys\n    import json\n    import pickle\n    from pathlib import\
          \ Path\n\n    print(f\"\U0001F504 Executing data_loading component...\"\
          )\n    print(f\"\U0001F4DD Original cell content from notebook cell 4\"\
          )\n\n    # Original cell code\n    # Data Loading and Initial Exploration\n\
          \    import pandas as pd\n    import numpy as np\n    from sklearn.datasets\
          \ import load_iris\n    import matplotlib.pyplot as plt\n\n    # Load the\
          \ iris dataset\n    print(\"\U0001F504 Loading iris dataset...\")\n    iris\
          \ = load_iris()\n\n    # Create DataFrame\n    df = pd.DataFrame(iris.data,\
          \ columns=iris.feature_names)\n    df['target'] = iris.target\n    df['target_name']\
          \ = df['target'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n\n\
          \    print(f\"\u2705 Dataset loaded successfully!\")\n    print(f\"\U0001F4CA\
          \ Dataset shape: {df.shape}\")\n    print(f\"\U0001F3F7\uFE0F Features:\
          \ {list(df.columns[:-2])}\")\n    print(f\"\U0001F3AF Target classes: {df['target_name'].unique()}\"\
          )\n\n    # Basic statistics\n    print(\"\\n\U0001F4C8 Dataset Overview:\"\
          )\n    print(df.describe())\n\n    # Save some key metrics\n    total_samples\
          \ = len(df)\n    num_features = len(iris.feature_names)\n    num_classes\
          \ = len(iris.target_names)\n\n    print(f\"\\n\U0001F4CB Summary: {total_samples}\
          \ samples, {num_features} features, {num_classes} classes\")\n\n    print(f\"\
          \u2705 data_loading component execution completed\")\n\n    # Return results\n\
          \    from collections import namedtuple\n    Outputs = namedtuple('Outputs',\
          \ ['dataset', 'metrics', 'artifacts'])\n    return Outputs(locals().get(\"\
          dataset\", f\"Missing: dataset\"), locals().get(\"metrics\", f\"Missing:\
          \ metrics\"), locals().get(\"artifacts\", f\"Missing: artifacts\"))\n\n"
        image: python:3.9
    exec-evaluation:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - evaluation
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'numpy' 'pandas'\
          \ 'scikit-learn' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef evaluation() -> NamedTuple('Outputs', [('model', Model), ('metrics',\
          \ Metrics)]):\n    \"\"\"\n    Pipeline step: step:evaluation\n    Generated\
          \ from notebook cell 7\n    Original tags: step:evaluation\n    \"\"\"\n\
          \    # Imports\n    import os\n    import sys\n    import json\n    import\
          \ pickle\n    from pathlib import Path\n\n    print(f\"\U0001F504 Executing\
          \ evaluation component...\")\n    print(f\"\U0001F4DD Original cell content\
          \ from notebook cell 7\")\n\n    # Original cell code\n    # Model Evaluation\
          \ and Performance Analysis\n    from sklearn.metrics import classification_report,\
          \ confusion_matrix\n    from sklearn.metrics import precision_score, recall_score,\
          \ f1_score\n    import numpy as np\n\n    print(\"\U0001F504 Starting model\
          \ evaluation...\")\n\n    # Evaluate all models\n    evaluation_results\
          \ = {}\n\n    for name, model in models.items():\n        print(f\"\\n\U0001F4CA\
          \ Evaluating {name}...\")\n\n        # Make predictions\n        y_pred\
          \ = model.predict(X_test_scaled)\n\n        # Calculate metrics\n      \
          \  accuracy = accuracy_score(y_test, y_pred)\n        precision = precision_score(y_test,\
          \ y_pred, average='weighted')\n        recall = recall_score(y_test, y_pred,\
          \ average='weighted')\n        f1 = f1_score(y_test, y_pred, average='weighted')\n\
          \n        # Store results\n        evaluation_results[name] = {\n      \
          \      'accuracy': accuracy,\n            'precision': precision,\n    \
          \        'recall': recall,\n            'f1_score': f1,\n            'training_time':\
          \ training_times[name]\n        }\n\n        print(f\"   \U0001F3AF Accuracy:\
          \ {accuracy:.4f}\")\n        print(f\"   \U0001F3AF Precision: {precision:.4f}\"\
          )\n        print(f\"   \U0001F3AF Recall: {recall:.4f}\")\n        print(f\"\
          \   \U0001F3AF F1-Score: {f1:.4f}\")\n\n    # Find best model\n    best_model_name\
          \ = max(evaluation_results, key=lambda x: evaluation_results[x]['accuracy'])\n\
          \    best_model = models[best_model_name]\n    best_accuracy = evaluation_results[best_model_name]['accuracy']\n\
          \n    print(f\"\\n\U0001F3C6 Best Model: {best_model_name}\")\n    print(f\"\
          \U0001F3C6 Best Accuracy: {best_accuracy:.4f}\")\n\n    # Detailed evaluation\
          \ of best model\n    print(f\"\\n\U0001F4CB Detailed Classification Report\
          \ for {best_model_name}:\")\n    y_pred_best = best_model.predict(X_test_scaled)\n\
          \    print(classification_report(y_test, y_pred_best, target_names=iris.target_names))\n\
          \n    # Confusion matrix\n    conf_matrix = confusion_matrix(y_test, y_pred_best)\n\
          \    print(f\"\\n\U0001F50D Confusion Matrix for {best_model_name}:\")\n\
          \    print(conf_matrix)\n\n    # Model comparison summary\n    print(f\"\
          \\n\U0001F4CA Model Comparison Summary:\")\n    for name, results in evaluation_results.items():\n\
          \        print(f\"{name:15} | Acc: {results['accuracy']:.4f} | Time: {results['training_time']:.3f}s\"\
          )\n\n    # Final metrics for pipeline output\n    final_accuracy = best_accuracy\n\
          \    final_model_name = best_model_name\n    total_models_trained = len(models)\n\
          \n    print(f\"\u2705 evaluation component execution completed\")\n\n  \
          \  # Return results\n    from collections import namedtuple\n    Outputs\
          \ = namedtuple('Outputs', ['model', 'metrics'])\n    return Outputs(locals().get(\"\
          model\", f\"Missing: model\"), locals().get(\"metrics\", f\"Missing: metrics\"\
          ))\n\n"
        image: python:3.9
    exec-pipeline-metrics-9:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - pipeline_metrics_9
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'numpy' 'pandas'\
          \ 'scikit-learn' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef pipeline_metrics_9() -> NamedTuple('Metrics', [('metrics', dict)]):\n\
          \    \"\"\"\n    Pipeline metrics calculation\n    Generated from notebook\
          \ cell 9\n    \"\"\"\n    import json\n    from typing import NamedTuple\n\
          \n    print(f\"\U0001F504 Computing metrics from cell 9...\")\n\n    # Pipeline\
          \ Results and Metrics Summary\n    print(\"\U0001F504 Generating pipeline\
          \ metrics summary...\")\n\n    # Calculate overall pipeline metrics\n  \
          \  pipeline_success = True\n    total_execution_time = sum(training_times.values())\n\
          \n    # Performance metrics\n    avg_accuracy = np.mean([results['accuracy']\
          \ for results in evaluation_results.values()])\n    std_accuracy = np.std([results['accuracy']\
          \ for results in evaluation_results.values()])\n\n    # Model diversity\n\
          \    accuracy_range = max(evaluation_results.values(), key=lambda x: x['accuracy'])['accuracy']\
          \ - \\\n                    min(evaluation_results.values(), key=lambda\
          \ x: x['accuracy'])['accuracy']\n\n    # Final pipeline metrics\n    pipeline_metrics\
          \ = {\n        'pipeline_success': pipeline_success,\n        'best_model':\
          \ best_model_name,\n        'best_accuracy': float(best_accuracy),\n   \
          \     'average_accuracy': float(avg_accuracy),\n        'accuracy_std':\
          \ float(std_accuracy),\n        'accuracy_range': float(accuracy_range),\n\
          \        'total_models': total_models_trained,\n        'total_training_time':\
          \ float(total_execution_time),\n        'dataset_size': total_samples,\n\
          \        'test_samples': len(X_test),\n        'num_features': num_features,\n\
          \        'num_classes': num_classes\n    }\n\n    print(\"\u2705 Pipeline\
          \ metrics computed:\")\n    print(f\"   \U0001F3C6 Best model: {best_model_name}\
          \ ({best_accuracy:.4f})\")\n    print(f\"   \U0001F4CA Average accuracy:\
          \ {avg_accuracy:.4f} \xB1 {std_accuracy:.4f}\")\n    print(f\"   \u23F1\uFE0F\
          \ Total training time: {total_execution_time:.3f}s\")\n    print(f\"   \U0001F522\
          \ Models trained: {total_models_trained}\")\n    print(f\"   \U0001F4C8\
          \ Accuracy range: {accuracy_range:.4f}\")\n\n    # Success criteria\n  \
          \  success_threshold = 0.90\n    pipeline_passed = best_accuracy >= success_threshold\n\
          \n    print(f\"\\n\U0001F3AF Pipeline Quality Assessment:\")\n    print(f\"\
          \   Success threshold: {success_threshold}\")\n    print(f\"   Pipeline\
          \ passed: {'\u2705 YES' if pipeline_passed else '\u274C NO'}\")\n\n    if\
          \ pipeline_passed:\n        print(\"\U0001F389 Pipeline completed successfully\
          \ with high accuracy!\")\n    else:\n        print(\"\u26A0\uFE0F Pipeline\
          \ completed but accuracy below threshold\")\n\n    # Extract metrics from\
          \ local variables\n    metrics = {k: v for k, v in locals().items() \n \
          \             if not k.startswith('_') and not callable(v) and \n      \
          \        isinstance(v, (int, float, str, bool)) and \n              k not\
          \ in ['json', 'NamedTuple', 'print']}\n\n    print(f\"\U0001F4CA Computed\
          \ metrics: {list(metrics.keys())}\")\n    print(\"\u2705 Metrics computation\
          \ completed\")\n\n    from collections import namedtuple\n    Metrics =\
          \ namedtuple('Metrics', ['metrics'])\n    return Metrics(metrics)\n\n"
        image: python:3.9
    exec-pipeline-parameters-8:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - pipeline_parameters_8
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'numpy' 'pandas'\
          \ 'scikit-learn' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef pipeline_parameters_8() -> dict:\n    \"\"\"\n    Pipeline parameters\
          \ configuration\n    Generated from notebook cell 8\n    \"\"\"\n    import\
          \ json\n\n    print(f\"\U0001F504 Loading pipeline parameters from cell\
          \ 8...\")\n\n    # Pipeline Configuration Parameters\n    print(\"\U0001F504\
          \ Setting up pipeline parameters...\")\n\n    # Data parameters\n    dataset_name\
          \ = \"iris\"\n    target_column = \"target\"\n    test_size_param = 0.2\n\
          \    random_state_param = 42\n\n    # Model parameters\n    rf_n_estimators\
          \ = 100\n    rf_max_depth = 5\n    lr_max_iter = 1000\n    svm_kernel =\
          \ \"rbf\"\n\n    # Evaluation parameters\n    scoring_metric = \"accuracy\"\
          \n    cv_folds = 5\n\n    # Pipeline metadata\n    pipeline_version = \"\
          1.0.0\"\n    pipeline_description = \"Iris classification pipeline with\
          \ multiple models\"\n    author = \"Pipeline Builder Extension\"\n\n   \
          \ print(\"\u2705 Pipeline parameters configured:\")\n    print(f\"   \U0001F4CA\
          \ Dataset: {dataset_name}\")\n    print(f\"   \U0001F3AF Target: {target_column}\"\
          )\n    print(f\"   \U0001F4C8 Test size: {test_size_param}\")\n    print(f\"\
          \   \U0001F522 Random state: {random_state_param}\")\n    print(f\"   \U0001F4CB\
          \ Version: {pipeline_version}\")\n\n    # Extract parameters from local\
          \ variables\n    params = {k: v for k, v in locals().items() \n        \
          \     if not k.startswith('_') and not callable(v) and k not in ['json',\
          \ 'print']}\n\n    print(f\"\U0001F4CB Loaded parameters: {list(params.keys())}\"\
          )\n    print(\"\u2705 Parameters loaded successfully\")\n\n    return params\n\
          \n"
        image: python:3.9
    exec-preprocessing:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - preprocessing
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'numpy' 'pandas'\
          \ 'scikit-learn' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef preprocessing() -> str:\n    \"\"\"\n    Pipeline step: step:preprocessing\n\
          \    Generated from notebook cell 5\n    Original tags: step:preprocessing\n\
          \    \"\"\"\n    # Imports\n    import os\n    import sys\n    import json\n\
          \    import pickle\n    from pathlib import Path\n\n    print(f\"\U0001F504\
          \ Executing preprocessing component...\")\n    print(f\"\U0001F4DD Original\
          \ cell content from notebook cell 5\")\n\n    # Original cell code\n   \
          \ # Data Preprocessing and Feature Engineering\n    from sklearn.model_selection\
          \ import train_test_split\n    from sklearn.preprocessing import StandardScaler\n\
          \    from sklearn.preprocessing import LabelEncoder\n\n    print(\"\U0001F504\
          \ Starting data preprocessing...\")\n\n    # Prepare features and target\n\
          \    X = df.drop(['target', 'target_name'], axis=1)\n    y = df['target']\n\
          \n    print(f\"\U0001F4CA Features shape: {X.shape}\")\n    print(f\"\U0001F3AF\
          \ Target shape: {y.shape}\")\n\n    # Split the data\n    test_size = 0.2\n\
          \    random_state = 42\n\n    X_train, X_test, y_train, y_test = train_test_split(\n\
          \        X, y, test_size=test_size, random_state=random_state, stratify=y\n\
          \    )\n\n    print(f\"\U0001F4C8 Training set: {X_train.shape[0]} samples\"\
          )\n    print(f\"\U0001F4C9 Test set: {X_test.shape[0]} samples\")\n\n  \
          \  # Feature scaling\n    scaler = StandardScaler()\n    X_train_scaled\
          \ = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n\
          \n    print(\"\U0001F527 Feature scaling completed\")\n\n    # Check for\
          \ missing values\n    missing_values = df.isnull().sum().sum()\n    print(f\"\
          \U0001F50D Missing values: {missing_values}\")\n\n    # Feature statistics\
          \ after scaling\n    train_mean = np.mean(X_train_scaled, axis=0)\n    train_std\
          \ = np.std(X_train_scaled, axis=0)\n\n    print(\"\u2705 Preprocessing completed\
          \ successfully!\")\n    print(f\"\U0001F4CA Scaled features - Mean: {train_mean.round(3)}\"\
          )\n    print(f\"\U0001F4CA Scaled features - Std: {train_std.round(3)}\"\
          )\n\n    preprocessing_summary = {\n        'train_samples': len(X_train),\n\
          \        'test_samples': len(X_test),\n        'features': X_train.shape[1],\n\
          \        'test_size_ratio': test_size\n    }\n\n    print(f\"\u2705 preprocessing\
          \ component execution completed\")\n\n    # Return results\n    # Try to\
          \ return meaningful output from the executed code\n    result_vars = [k\
          \ for k in locals().keys() if not k.startswith('_') and not callable(locals()[k])]\n\
          \    if result_vars:\n        # Return a summary of what was created/computed\n\
          \        summary = f\"Executed successfully. Created variables: {', '.join(result_vars[:5])}\"\
          \n        if len(result_vars) > 5:\n            summary += f\" and {len(result_vars)\
          \ - 5} more\"\n        return summary\n    else:\n        return \"Step\
          \ completed successfully\"\n\n"
        image: python:3.9
    exec-training:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - training
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'numpy' 'pandas'\
          \ 'scikit-learn' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef training() -> NamedTuple('Outputs', [('model', Model), ('metrics',\
          \ Metrics)]):\n    \"\"\"\n    Pipeline step: step:training\n    Generated\
          \ from notebook cell 6\n    Original tags: step:training\n    \"\"\"\n \
          \   # Imports\n    import os\n    import sys\n    import json\n    import\
          \ pickle\n    from pathlib import Path\n\n    print(f\"\U0001F504 Executing\
          \ training component...\")\n    print(f\"\U0001F4DD Original cell content\
          \ from notebook cell 6\")\n\n    # Original cell code\n    # Model Training\n\
          \    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.linear_model\
          \ import LogisticRegression\n    from sklearn.svm import SVC\n    from sklearn.metrics\
          \ import accuracy_score\n    import time\n\n    print(\"\U0001F504 Starting\
          \ model training...\")\n\n    # Define model parameters\n    rf_params =\
          \ {\n        'n_estimators': 100,\n        'random_state': 42,\n       \
          \ 'max_depth': 5\n    }\n\n    lr_params = {\n        'random_state': 42,\n\
          \        'max_iter': 1000\n    }\n\n    svm_params = {\n        'random_state':\
          \ 42,\n        'kernel': 'rbf'\n    }\n\n    # Train multiple models\n \
          \   models = {}\n    training_times = {}\n\n    print(\"\U0001F332 Training\
          \ Random Forest...\")\n    start_time = time.time()\n    rf_model = RandomForestClassifier(**rf_params)\n\
          \    rf_model.fit(X_train_scaled, y_train)\n    training_times['RandomForest']\
          \ = time.time() - start_time\n    models['RandomForest'] = rf_model\n  \
          \  print(f\"   \u23F1\uFE0F Training time: {training_times['RandomForest']:.3f}s\"\
          )\n\n    print(\"\U0001F4CA Training Logistic Regression...\")\n    start_time\
          \ = time.time()\n    lr_model = LogisticRegression(**lr_params)\n    lr_model.fit(X_train_scaled,\
          \ y_train)\n    training_times['LogisticRegression'] = time.time() - start_time\n\
          \    models['LogisticRegression'] = lr_model\n    print(f\"   \u23F1\uFE0F\
          \ Training time: {training_times['LogisticRegression']:.3f}s\")\n\n    print(\"\
          \U0001F3AF Training SVM...\")\n    start_time = time.time()\n    svm_model\
          \ = SVC(**svm_params)\n    svm_model.fit(X_train_scaled, y_train)\n    training_times['SVM']\
          \ = time.time() - start_time\n    models['SVM'] = svm_model\n    print(f\"\
          \   \u23F1\uFE0F Training time: {training_times['SVM']:.3f}s\")\n\n    print(\"\
          \u2705 All models trained successfully!\")\n    print(f\"\U0001F527 Trained\
          \ {len(models)} models: {list(models.keys())}\")\n\n    # Quick training\
          \ accuracy check\n    train_accuracies = {}\n    for name, model in models.items():\n\
          \        train_pred = model.predict(X_train_scaled)\n        train_acc =\
          \ accuracy_score(y_train, train_pred)\n        train_accuracies[name] =\
          \ train_acc\n        print(f\"\U0001F4C8 {name} training accuracy: {train_acc:.4f}\"\
          )\n\n    best_train_model = max(train_accuracies, key=train_accuracies.get)\n\
          \    print(f\"\U0001F3C6 Best training accuracy: {best_train_model} ({train_accuracies[best_train_model]:.4f})\"\
          )\n\n    print(f\"\u2705 training component execution completed\")\n\n \
          \   # Return results\n    from collections import namedtuple\n    Outputs\
          \ = namedtuple('Outputs', ['model', 'metrics'])\n    return Outputs(locals().get(\"\
          model\", f\"Missing: model\"), locals().get(\"metrics\", f\"Missing: metrics\"\
          ))\n\n"
        image: python:3.9
pipelineInfo:
  description: Generated from annotated notebook
  name: iris-example
root:
  dag:
    tasks:
      data-loading:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-data-loading
        taskInfo:
          name: data-loading
      evaluation:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-evaluation
        dependentTasks:
        - training
        taskInfo:
          name: evaluation
      pipeline-metrics-9:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-pipeline-metrics-9
        dependentTasks:
        - pipeline-parameters-8
        taskInfo:
          name: pipeline-metrics-9
      pipeline-parameters-8:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-pipeline-parameters-8
        dependentTasks:
        - evaluation
        taskInfo:
          name: pipeline-parameters-8
      preprocessing:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-preprocessing
        dependentTasks:
        - data-loading
        taskInfo:
          name: preprocessing
      training:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-training
        dependentTasks:
        - preprocessing
        taskInfo:
          name: training
schemaVersion: 2.1.0
sdkVersion: kfp-2.13.0
