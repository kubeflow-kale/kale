{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxi Cab Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents a simplified version of Kubeflow's *taxi cab clasification* pipeline, built upon TFX components.\n",
    "\n",
    "Here all the pipeline components are stripped down to their core to showcase how to run it in a self-contained local Juyter Noteobok.\n",
    "\n",
    "Additionally, the pipeline has been upgraded to work with Python3 and all major libraries (Tensorflow, Tensorflow Transform, Tensorflow Model Analysis, Tensorflow Data Validation, Apache Beam) have been bumped to their latests versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "imports"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "import apache_beam as beam\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow_model_analysis as tfma\n",
    "import tensorflow_data_validation as tfdv\n",
    "\n",
    "from apache_beam.io import textio\n",
    "from apache_beam.io import tfrecordio\n",
    "\n",
    "from tensorflow_transform.beam import impl as beam_impl\n",
    "from tensorflow_transform.beam.tft_beam_io import transform_fn_io\n",
    "from tensorflow_transform.coders.csv_coder import CsvCoder\n",
    "from tensorflow_transform.coders.example_proto_coder import ExampleProtoCoder\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "from tensorflow_transform.tf_metadata import metadata_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "functions"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.6/site-packages/apache_beam/__init__.py:84: UserWarning: Some syntactic constructs of Python 3 are not yet fully supported by Apache Beam.\n",
      "  'Some syntactic constructs of Python 3 are not yet fully supported by '\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = 'data/'\n",
    "TRAIN_DATA = os.path.join(DATA_DIR, 'taxi-cab-classification/train.csv')\n",
    "EVALUATION_DATA = os.path.join(DATA_DIR, 'taxi-cab-classification/eval.csv')\n",
    "\n",
    "# Categorical features are assumed to each have a maximum value in the dataset.\n",
    "MAX_CATEGORICAL_FEATURE_VALUES = [24, 31, 12]\n",
    "CATEGORICAL_FEATURE_KEYS = ['trip_start_hour', 'trip_start_day', 'trip_start_month']\n",
    "\n",
    "DENSE_FLOAT_FEATURE_KEYS = ['trip_miles', 'fare', 'trip_seconds']\n",
    "\n",
    "# Number of buckets used by tf.transform for encoding each feature.\n",
    "FEATURE_BUCKET_COUNT = 10\n",
    "\n",
    "BUCKET_FEATURE_KEYS = ['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude']\n",
    "\n",
    "# Number of vocabulary terms used for encoding VOCAB_FEATURES by tf.transform\n",
    "VOCAB_SIZE = 1000\n",
    "\n",
    "# Count of out-of-vocab buckets in which unrecognized VOCAB_FEATURES are hashed.\n",
    "OOV_SIZE = 10\n",
    "\n",
    "VOCAB_FEATURE_KEYS = ['pickup_census_tract', 'dropoff_census_tract', 'payment_type', 'company',\n",
    "    'pickup_community_area', 'dropoff_community_area']\n",
    "\n",
    "# allow nan values in these features.\n",
    "OPTIONAL_FEATURES = ['dropoff_latitude', 'dropoff_longitude', 'pickup_census_tract', 'dropoff_census_tract',\n",
    "    'company', 'trip_seconds', 'dropoff_community_area']\n",
    "\n",
    "LABEL_KEY = 'tips'\n",
    "FARE_KEY = 'fare'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "pipeline-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# training parameters\n",
    "EPOCHS = 1\n",
    "STEPS = 3\n",
    "BATCH_SIZE = 32\n",
    "HIDDEN_LAYER_SIZE = '1500'\n",
    "LEARNING_RATE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "functions"
    ]
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n",
    "# tf.get_logger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an overview of the TFDV functions: https://www.tensorflow.org/tfx/tutorials/data_validation/chicago_taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "block:data_validation"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1107 15:33:09.759347 140276907915072 deprecation_wrapper.py:119] From /home/jovyan/.local/lib/python3.6/site-packages/tensorflow_data_validation/utils/stats_gen_lib.py:165: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n",
      "W1107 15:33:09.767974 140276907915072 deprecation_wrapper.py:119] From /home/jovyan/.local/lib/python3.6/site-packages/tensorflow_data_validation/utils/stats_gen_lib.py:321: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
      "\n",
      "W1107 15:33:09.790357 140276907915072 deprecation_wrapper.py:119] From /home/jovyan/.local/lib/python3.6/site-packages/tensorflow_data_validation/utils/stats_gen_lib.py:327: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "W1107 15:33:17.073589 140274294482688 tfrecordio.py:57] Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n",
      "W1107 15:33:17.253429 140276907915072 deprecation.py:323] From /home/jovyan/.local/lib/python3.6/site-packages/tensorflow_data_validation/utils/stats_gen_lib.py:357: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "E1107 15:33:22.659946 140276907915072 <ipython-input-4-b1cbdc85381a>:22] Anomaly in feature \"company\": Examples contain values missing from the schema: 2092 - 61288 Sbeih company (<1%), 2192 - 73487 Zeymane Corp (<1%), 2192 - Zeymane Corp (<1%), 2823 - 73307 Seung Lee (<1%), 3094 - 24059 G.L.B. Cab Co (<1%), 3319 - CD Cab Co (<1%), 3385 - Eman Cab (<1%), 3897 - 57856 Ilie Malec (<1%), 4053 - 40193 Adwar H. Nikola (<1%), 4197 - Royal Star (<1%), 585 - 88805 Valley Cab Co (<1%), 5874 - Sergey Cab Corp. (<1%), 6057 - 24657 Richard Addo (<1%), 6574 - Babylon Express Inc. (<1%), 6742 - 83735 Tasha ride inc (<1%). \n",
      "E1107 15:33:22.662289 140276907915072 <ipython-input-4-b1cbdc85381a>:22] Anomaly in feature \"payment_type\": Examples contain values missing from the schema: Prcard (<1%). \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Presence</th>\n",
       "      <th>Valency</th>\n",
       "      <th>Domain</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>'trip_start_hour'</td>\n",
       "      <td>INT</td>\n",
       "      <td>required</td>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>'trip_miles'</td>\n",
       "      <td>FLOAT</td>\n",
       "      <td>required</td>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>'dropoff_census_tract'</td>\n",
       "      <td>FLOAT</td>\n",
       "      <td>optional</td>\n",
       "      <td>single</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>'trip_seconds'</td>\n",
       "      <td>FLOAT</td>\n",
       "      <td>optional</td>\n",
       "      <td>single</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>'pickup_community_area'</td>\n",
       "      <td>INT</td>\n",
       "      <td>required</td>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>'tips'</td>\n",
       "      <td>FLOAT</td>\n",
       "      <td>required</td>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>'payment_type'</td>\n",
       "      <td>STRING</td>\n",
       "      <td>required</td>\n",
       "      <td></td>\n",
       "      <td>'payment_type'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>'trip_start_timestamp'</td>\n",
       "      <td>INT</td>\n",
       "      <td>required</td>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>'dropoff_latitude'</td>\n",
       "      <td>FLOAT</td>\n",
       "      <td>optional</td>\n",
       "      <td>single</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>'trip_start_month'</td>\n",
       "      <td>INT</td>\n",
       "      <td>required</td>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>'dropoff_longitude'</td>\n",
       "      <td>FLOAT</td>\n",
       "      <td>optional</td>\n",
       "      <td>single</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>'trip_start_day'</td>\n",
       "      <td>INT</td>\n",
       "      <td>required</td>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>'pickup_latitude'</td>\n",
       "      <td>FLOAT</td>\n",
       "      <td>required</td>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>'pickup_longitude'</td>\n",
       "      <td>FLOAT</td>\n",
       "      <td>required</td>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>'fare'</td>\n",
       "      <td>FLOAT</td>\n",
       "      <td>required</td>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>'company'</td>\n",
       "      <td>STRING</td>\n",
       "      <td>optional</td>\n",
       "      <td>single</td>\n",
       "      <td>'company'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>'pickup_census_tract'</td>\n",
       "      <td>BYTES</td>\n",
       "      <td>optional</td>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>'dropoff_community_area'</td>\n",
       "      <td>FLOAT</td>\n",
       "      <td>optional</td>\n",
       "      <td>single</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Type  Presence Valency          Domain\n",
       "Feature name                                                      \n",
       "'trip_start_hour'            INT  required                       -\n",
       "'trip_miles'               FLOAT  required                       -\n",
       "'dropoff_census_tract'     FLOAT  optional  single               -\n",
       "'trip_seconds'             FLOAT  optional  single               -\n",
       "'pickup_community_area'      INT  required                       -\n",
       "'tips'                     FLOAT  required                       -\n",
       "'payment_type'            STRING  required          'payment_type'\n",
       "'trip_start_timestamp'       INT  required                       -\n",
       "'dropoff_latitude'         FLOAT  optional  single               -\n",
       "'trip_start_month'           INT  required                       -\n",
       "'dropoff_longitude'        FLOAT  optional  single               -\n",
       "'trip_start_day'             INT  required                       -\n",
       "'pickup_latitude'          FLOAT  required                       -\n",
       "'pickup_longitude'         FLOAT  required                       -\n",
       "'fare'                     FLOAT  required                       -\n",
       "'company'                 STRING  optional  single       'company'\n",
       "'pickup_census_tract'      BYTES  optional                       -\n",
       "'dropoff_community_area'   FLOAT  optional  single               -"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Values</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Domain</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>'payment_type'</td>\n",
       "      <td>'Cash', 'Credit Card', 'Dispute', 'No Charge', 'Pcard', 'Unknown'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>'company'</td>\n",
       "      <td>'0118 - 42111 Godfrey S.Awir', '0694 - 59280 Chinesco Trans Inc', '1085 - 72312 N and W Cab Co', '2733 - 74600 Benny Jona', '2809 - 95474 C &amp; D Cab Co Inc.', '3011 - 66308 JBL Cab Inc.', '3152 - 97284 Crystal Abernathy', '3201 - C&amp;D Cab Co Inc', '3201 - CID Cab Co Inc', '3253 - 91138 Gaither Cab Co.', '3385 - 23210 Eman Cab', '3623 - 72222 Arrington Enterprises', '3897 - Ilie Malec', '4053 - Adwar H. Nikola', '4197 - 41842 Royal Star', '4615 - 83503 Tyrone Henderson', '4615 - Tyrone Henderson', '4623 - Jay Kim', '5006 - 39261 Salifu Bawa', '5006 - Salifu Bawa', '5074 - 54002 Ahzmi Inc', '5074 - Ahzmi Inc', '5129 - 87128', '5129 - 98755 Mengisti Taxi', '5129 - Mengisti Taxi', '5724 - KYVI Cab Inc', '585 - Valley Cab Co', '5864 - 73614 Thomas Owusu', '5864 - Thomas Owusu', '5874 - 73628 Sergey Cab Corp.', '5997 - 65283 AW Services Inc.', '5997 - AW Services Inc.', '6488 - 83287 Zuha Taxi', '6743 - Luhak Corp', 'Blue Ribbon Taxi Association Inc.', 'C &amp; D Cab Co Inc', 'Chicago Elite Cab Corp.', 'Chicago Elite Cab Corp. (Chicago Carriag', 'Chicago Medallion Leasing INC', 'Chicago Medallion Management', 'Choice Taxi Association', 'Dispatch Taxi Affiliation', 'KOAM Taxi Association', 'Northwest Management LLC', 'Taxi Affiliation Services', 'Top Cab Affiliation'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Values\n",
       "Domain                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "'payment_type'  'Cash', 'Credit Card', 'Dispute', 'No Charge', 'Pcard', 'Unknown'                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "'company'       '0118 - 42111 Godfrey S.Awir', '0694 - 59280 Chinesco Trans Inc', '1085 - 72312 N and W Cab Co', '2733 - 74600 Benny Jona', '2809 - 95474 C & D Cab Co Inc.', '3011 - 66308 JBL Cab Inc.', '3152 - 97284 Crystal Abernathy', '3201 - C&D Cab Co Inc', '3201 - CID Cab Co Inc', '3253 - 91138 Gaither Cab Co.', '3385 - 23210 Eman Cab', '3623 - 72222 Arrington Enterprises', '3897 - Ilie Malec', '4053 - Adwar H. Nikola', '4197 - 41842 Royal Star', '4615 - 83503 Tyrone Henderson', '4615 - Tyrone Henderson', '4623 - Jay Kim', '5006 - 39261 Salifu Bawa', '5006 - Salifu Bawa', '5074 - 54002 Ahzmi Inc', '5074 - Ahzmi Inc', '5129 - 87128', '5129 - 98755 Mengisti Taxi', '5129 - Mengisti Taxi', '5724 - KYVI Cab Inc', '585 - Valley Cab Co', '5864 - 73614 Thomas Owusu', '5864 - Thomas Owusu', '5874 - 73628 Sergey Cab Corp.', '5997 - 65283 AW Services Inc.', '5997 - AW Services Inc.', '6488 - 83287 Zuha Taxi', '6743 - Luhak Corp', 'Blue Ribbon Taxi Association Inc.', 'C & D Cab Co Inc', 'Chicago Elite Cab Corp.', 'Chicago Elite Cab Corp. (Chicago Carriag', 'Chicago Medallion Leasing INC', 'Chicago Medallion Management', 'Choice Taxi Association', 'Dispatch Taxi Affiliation', 'KOAM Taxi Association', 'Northwest Management LLC', 'Taxi Affiliation Services', 'Top Cab Affiliation'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vldn_output = os.path.join(DATA_DIR, 'validation')\n",
    "\n",
    "# TODO: Understand why this was used in the conversion to the output json\n",
    "# key columns: list of the names for columns that should be treated as unique keys.\n",
    "key_columns = ['trip_start_timestamp']\n",
    "\n",
    "# read the first line of the cvs to have and ordered list of column names \n",
    "# (the Schema will scrable the features)\n",
    "with open(TRAIN_DATA) as f:\n",
    "    column_names = f.readline().strip().split(',')\n",
    "\n",
    "stats = tfdv.generate_statistics_from_csv(data_location=TRAIN_DATA)\n",
    "schema = tfdv.infer_schema(stats)\n",
    "\n",
    "eval_stats = tfdv.generate_statistics_from_csv(data_location=EVALUATION_DATA)\n",
    "anomalies = tfdv.validate_statistics(eval_stats, schema)\n",
    "\n",
    "# Log anomalies\n",
    "for feature_name, anomaly_info in anomalies.anomaly_info.items():\n",
    "    logging.getLogger().error(\n",
    "        'Anomaly in feature \"{}\": {}'.format(\n",
    "            feature_name, anomaly_info.description))\n",
    "    \n",
    "# show inferred schema\n",
    "tfdv.display_schema(schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4 style=\"color:green;\">No anomalies found.</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Resolve anomalies\n",
    "company = tfdv.get_feature(schema, 'company')\n",
    "company.distribution_constraints.min_domain_mass = 0.9\n",
    "\n",
    "# Add new value to the domain of feature payment_type.\n",
    "payment_type_domain = tfdv.get_domain(schema, 'payment_type')\n",
    "payment_type_domain.value.append('Prcard')\n",
    "\n",
    "# Validate eval stats after updating the schema \n",
    "updated_anomalies = tfdv.validate_statistics(eval_stats, schema)\n",
    "tfdv.display_anomalies(updated_anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an overview of the TFT functions: https://www.tensorflow.org/tfx/tutorials/transform/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "block:data_transformation",
     "prev:data_validation"
    ]
   },
   "outputs": [],
   "source": [
    "def to_dense(tensor):\n",
    "    \"\"\"Takes as input a SparseTensor and return a Tensor with correct default value\n",
    "    Args:\n",
    "      tensor: tf.SparseTensor\n",
    "    Returns:\n",
    "      tf.Tensor with default value\n",
    "    \"\"\"\n",
    "    if not isinstance(tensor, tf.sparse.SparseTensor):\n",
    "        return tensor\n",
    "    if tensor.dtype == tf.string:\n",
    "        default_value = ''\n",
    "    elif tensor.dtype == tf.float32:\n",
    "        default_value = 0.0\n",
    "    elif tensor.dtype == tf.int32:\n",
    "        default_value = 0\n",
    "    else:\n",
    "        raise ValueError(f\"Tensor type not recognized: {tensor.dtype}\")\n",
    "\n",
    "    return tf.squeeze(tf.sparse_to_dense(tensor.indices,\n",
    "                               [tensor.dense_shape[0], 1],\n",
    "                               tensor.values, default_value=default_value), axis=1)\n",
    "    # TODO: Update to below version\n",
    "    # return tf.squeeze(tf.sparse.to_dense(tensor, default_value=default_value), axis=1)\n",
    "\n",
    "\n",
    "def preprocess_fn(inputs):\n",
    "    \"\"\"tf.transform's callback function for preprocessing inputs.\n",
    "    Args:\n",
    "      inputs: map from feature keys to raw not-yet-transformed features.\n",
    "    Returns:\n",
    "      Map from string feature key to transformed feature operations.\n",
    "    \"\"\"\n",
    "    outputs = {}\n",
    "    for key in DENSE_FLOAT_FEATURE_KEYS:\n",
    "        # Preserve this feature as a dense float, setting nan's to the mean.\n",
    "        outputs[key] = tft.scale_to_z_score(to_dense(inputs[key]))\n",
    "\n",
    "    for key in VOCAB_FEATURE_KEYS:\n",
    "        # Build a vocabulary for this feature.\n",
    "        if inputs[key].dtype == tf.string:\n",
    "            vocab_tensor = to_dense(inputs[key])\n",
    "        else:\n",
    "            vocab_tensor = tf.as_string(to_dense(inputs[key]))\n",
    "        outputs[key] = tft.compute_and_apply_vocabulary(\n",
    "            vocab_tensor, vocab_filename='vocab_' + key,\n",
    "            top_k=VOCAB_SIZE, num_oov_buckets=OOV_SIZE)\n",
    "\n",
    "    for key in BUCKET_FEATURE_KEYS:\n",
    "        outputs[key] = tft.bucketize(to_dense(inputs[key]), FEATURE_BUCKET_COUNT)\n",
    "\n",
    "    for key in CATEGORICAL_FEATURE_KEYS:\n",
    "        outputs[key] = tf.cast(to_dense(inputs[key]), tf.int64)\n",
    "\n",
    "    taxi_fare = to_dense(inputs[FARE_KEY])\n",
    "    taxi_tip = to_dense(inputs[LABEL_KEY])\n",
    "    # Test if the tip was > 20% of the fare.\n",
    "    tip_threshold = tf.multiply(taxi_fare, tf.constant(0.2))\n",
    "    outputs[LABEL_KEY] = tf.logical_and(\n",
    "        tf.logical_not(tf.math.is_nan(taxi_fare)),\n",
    "        tf.greater(taxi_tip, tip_threshold))\n",
    "\n",
    "    for key in outputs:\n",
    "        if outputs[key].dtype == tf.bool:\n",
    "            outputs[key] = tft.compute_and_apply_vocabulary(tf.as_string(outputs[key]),\n",
    "                                             vocab_filename='vocab_' + key)\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1107 15:33:45.261063 140276907915072 deprecation.py:323] From /home/jovyan/.local/lib/python3.6/site-packages/tensorflow_transform/mappers.py:349: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W1107 15:33:45.289230 140276907915072 deprecation.py:323] From <ipython-input-6-d55a3ea73154>:21: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "W1107 15:33:45.652244 140276907915072 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "I1107 15:33:45.655295 140276907915072 builder_impl.py:661] Assets added to graph.\n",
      "I1107 15:33:45.656082 140276907915072 builder_impl.py:456] No assets to write.\n",
      "I1107 15:33:45.674907 140276907915072 builder_impl.py:421] SavedModel written to: data/transformed/tmp/tftransform_tmp/ada98794525d4bc7a625c4b64ae48fb6/saved_model.pb\n",
      "I1107 15:33:48.630603 140276907915072 builder_impl.py:661] Assets added to graph.\n",
      "I1107 15:33:48.631559 140276907915072 builder_impl.py:456] No assets to write.\n",
      "I1107 15:33:48.651397 140276907915072 builder_impl.py:421] SavedModel written to: data/transformed/tmp/tftransform_tmp/d558620a5a9d4347baaa7672bcfba03a/saved_model.pb\n",
      "I1107 15:33:55.861992 140274031982336 saver.py:1499] Saver not created because there are no variables in the graph to restore\n",
      "I1107 15:34:04.166496 140273799640832 saver.py:1499] Saver not created because there are no variables in the graph to restore\n",
      "I1107 15:34:04.183343 140273799640832 builder_impl.py:661] Assets added to graph.\n",
      "I1107 15:34:04.186910 140273799640832 builder_impl.py:770] Assets written to: data/transformed/tmp/tftransform_tmp/fbb3fb136eb348eda4a4bb0de0806d92/assets\n",
      "I1107 15:34:04.212712 140273799640832 builder_impl.py:421] SavedModel written to: data/transformed/tmp/tftransform_tmp/fbb3fb136eb348eda4a4bb0de0806d92/saved_model.pb\n",
      "W1107 15:34:04.519586 140273799640832 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_1:0\\022\\022vocab_payment_type\"\n",
      "\n",
      "W1107 15:34:04.522127 140273799640832 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_3:0\\022\\031vocab_pickup_census_tract\"\n",
      "\n",
      "W1107 15:34:04.523182 140273799640832 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_4:0\\022\\rvocab_company\"\n",
      "\n",
      "W1107 15:34:04.525978 140273799640832 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_7:0\\022\\nvocab_tips\"\n",
      "\n",
      "W1107 15:34:04.526508 140273799640832 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_8:0\\022\\033vocab_pickup_community_area\"\n",
      "\n",
      "W1107 15:34:04.528073 140273799640832 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_9:0\\022\\032vocab_dropoff_census_tract\"\n",
      "\n",
      "W1107 15:34:04.528851 140273799640832 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\014\\n\\nConst_13:0\\022\\034vocab_dropoff_community_area\"\n",
      "\n",
      "I1107 15:34:04.532896 140273799640832 saver.py:1499] Saver not created because there are no variables in the graph to restore\n",
      "W1107 15:34:04.780158 140273799640832 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_1:0\\022\\022vocab_payment_type\"\n",
      "\n",
      "W1107 15:34:04.781400 140273799640832 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_3:0\\022\\031vocab_pickup_census_tract\"\n",
      "\n",
      "W1107 15:34:04.783627 140273799640832 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_4:0\\022\\rvocab_company\"\n",
      "\n",
      "W1107 15:34:04.784431 140273799640832 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_7:0\\022\\nvocab_tips\"\n",
      "\n",
      "W1107 15:34:04.785658 140273799640832 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_8:0\\022\\033vocab_pickup_community_area\"\n",
      "\n",
      "W1107 15:34:04.787027 140273799640832 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_9:0\\022\\032vocab_dropoff_census_tract\"\n",
      "\n",
      "W1107 15:34:04.793606 140273799640832 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\014\\n\\nConst_13:0\\022\\034vocab_dropoff_community_area\"\n",
      "\n",
      "I1107 15:34:04.796040 140273799640832 saver.py:1499] Saver not created because there are no variables in the graph to restore\n",
      "W1107 15:34:06.475417 140273536583424 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_1:0\\022\\022vocab_payment_type\"\n",
      "\n",
      "W1107 15:34:06.478034 140273536583424 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_3:0\\022\\031vocab_pickup_census_tract\"\n",
      "\n",
      "W1107 15:34:06.479870 140273536583424 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_4:0\\022\\rvocab_company\"\n",
      "\n",
      "W1107 15:34:06.481529 140273536583424 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_7:0\\022\\nvocab_tips\"\n",
      "\n",
      "W1107 15:34:06.483190 140273536583424 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_8:0\\022\\033vocab_pickup_community_area\"\n",
      "\n",
      "W1107 15:34:06.484979 140273536583424 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_9:0\\022\\032vocab_dropoff_census_tract\"\n",
      "\n",
      "W1107 15:34:06.486701 140273536583424 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\014\\n\\nConst_13:0\\022\\034vocab_dropoff_community_area\"\n",
      "\n",
      "I1107 15:34:06.488938 140273536583424 saver.py:1499] Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "trns_output = os.path.join(DATA_DIR, \"transformed\")\n",
    "if os.path.exists(trns_output):\n",
    "    shutil.rmtree(trns_output)\n",
    "\n",
    "tft_input_metadata = dataset_metadata.DatasetMetadata(schema)\n",
    "\n",
    "runner = 'DirectRunner'\n",
    "with beam.Pipeline(runner, options=None) as p:\n",
    "    with beam_impl.Context(temp_dir=os.path.join(trns_output, 'tmp')):\n",
    "        converter = CsvCoder(column_names, tft_input_metadata.schema)\n",
    "\n",
    "        # READ TRAIN DATA\n",
    "        train_data = (\n",
    "                p\n",
    "                | 'ReadTrainData' >> textio.ReadFromText(TRAIN_DATA, skip_header_lines=1)\n",
    "                | 'DecodeTrainData' >> beam.Map(converter.decode))\n",
    "\n",
    "        # TRANSFORM TRAIN DATA (and get transform_fn function)\n",
    "        transformed_dataset, transform_fn = (\n",
    "                (train_data, tft_input_metadata) | beam_impl.AnalyzeAndTransformDataset(preprocess_fn))\n",
    "        transformed_data, transformed_metadata = transformed_dataset\n",
    "\n",
    "        # SAVE TRANSFORMED TRAIN DATA\n",
    "        _ = transformed_data | 'WriteTrainData' >> tfrecordio.WriteToTFRecord(\n",
    "            os.path.join(trns_output, 'train'),\n",
    "            coder=ExampleProtoCoder(transformed_metadata.schema))\n",
    "\n",
    "        # READ EVAL DATA\n",
    "        eval_data = (\n",
    "                p\n",
    "                | 'ReadEvalData' >> textio.ReadFromText(EVALUATION_DATA, skip_header_lines=1)\n",
    "                | 'DecodeEvalData' >> beam.Map(converter.decode))\n",
    "\n",
    "        # TRANSFORM EVAL DATA (using previously created transform_fn function)\n",
    "        eval_dataset = (eval_data, tft_input_metadata)\n",
    "        transformed_eval_data, transformed_metadata = (\n",
    "            (eval_dataset, transform_fn) | beam_impl.TransformDataset())\n",
    "\n",
    "        # SAVE EVAL DATA\n",
    "        _ = transformed_eval_data | 'WriteEvalData' >> tfrecordio.WriteToTFRecord(\n",
    "            os.path.join(trns_output, 'eval'),\n",
    "            coder=ExampleProtoCoder(transformed_metadata.schema))\n",
    "\n",
    "        # SAVE transform_fn FUNCTION FOR LATER USE\n",
    "        # TODO: check out what is the transform function (transform_fn) that came from previous step\n",
    "        _ = (transform_fn | 'WriteTransformFn' >> transform_fn_io.WriteTransformFn(trns_output))\n",
    "\n",
    "        # SAVE TRANSFORMED METADATA\n",
    "        metadata_io.write_metadata(\n",
    "            metadata=tft_input_metadata,\n",
    "            path=os.path.join(trns_output, 'metadata'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimator API: https://www.tensorflow.org/guide/premade_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "block:train",
     "prev:data_transformation"
    ]
   },
   "outputs": [],
   "source": [
    "def training_input_fn(transformed_output, transformed_examples, batch_size, target_name):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      transformed_output: tft.TFTransformOutput\n",
    "      transformed_examples: Base filename of examples\n",
    "      batch_size: Batch size.\n",
    "      target_name: name of the target column.\n",
    "    Returns:\n",
    "      The input function for training or eval.\n",
    "    \"\"\"\n",
    "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "        file_pattern=transformed_examples,\n",
    "        batch_size=batch_size,\n",
    "        features=transformed_output.transformed_feature_spec(),\n",
    "        reader=tf.data.TFRecordDataset,\n",
    "        shuffle=True)\n",
    "    transformed_features = dataset.make_one_shot_iterator().get_next()\n",
    "    transformed_labels = transformed_features.pop(target_name)\n",
    "    return transformed_features, transformed_labels\n",
    "\n",
    "\n",
    "def get_feature_columns():\n",
    "    \"\"\"Callback that returns a list of feature columns for building a tf.estimator.\n",
    "    Returns:\n",
    "      A list of tf.feature_column.\n",
    "    \"\"\"\n",
    "    return (\n",
    "            [tf.feature_column.numeric_column(key, shape=()) for key in DENSE_FLOAT_FEATURE_KEYS] +\n",
    "            [tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_identity(key, num_buckets=VOCAB_SIZE + OOV_SIZE)) for key in VOCAB_FEATURE_KEYS] +\n",
    "            [tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_identity(key, num_buckets=FEATURE_BUCKET_COUNT, default_value=0)) for key in BUCKET_FEATURE_KEYS] +\n",
    "            [tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_identity(key, num_buckets=num_buckets, default_value=0)) for key, num_buckets in zip(CATEGORICAL_FEATURE_KEYS, MAX_CATEGORICAL_FEATURE_VALUES)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1107 15:34:33.098996 140276907915072 estimator.py:209] Using config: {'_model_dir': 'data/training', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f941e831828>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "training_output = os.path.join(DATA_DIR, \"training\")\n",
    "if os.path.exists(training_output):\n",
    "    shutil.rmtree(training_output)\n",
    "\n",
    "hidden_layer_size = [int(x.strip()) for x in HIDDEN_LAYER_SIZE.split(',')]\n",
    "\n",
    "tf_transform_output = tft.TFTransformOutput(trns_output)\n",
    "\n",
    "# Set how often to run checkpointing in terms of steps.\n",
    "config = tf.estimator.RunConfig(save_checkpoints_steps=1000)\n",
    "n_classes = tf_transform_output.vocabulary_size_by_name(\"vocab_\" + LABEL_KEY)\n",
    "# Create estimator\n",
    "estimator =  tf.estimator.DNNClassifier(\n",
    "                feature_columns=get_feature_columns(),\n",
    "                hidden_units=hidden_layer_size,\n",
    "                n_classes=n_classes,\n",
    "                config=config,\n",
    "                model_dir=training_output)\n",
    "\n",
    "# TODO: Simplify all this: https://www.tensorflow.org/guide/premade_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1107 15:38:37.536054 140276907915072 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "W1107 15:38:37.555328 140276907915072 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/experimental/ops/readers.py:835: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
      "W1107 15:38:37.579628 140276907915072 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/experimental/ops/readers.py:212: shuffle_and_repeat (from tensorflow.python.data.experimental.ops.shuffle_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.shuffle(buffer_size, seed)` followed by `tf.data.Dataset.repeat(count)`. Static tf.data optimizations will take care of using the fused implementation.\n",
      "W1107 15:38:37.600170 140276907915072 deprecation.py:323] From <ipython-input-8-b5b04a71d5bc>:17: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "I1107 15:38:37.648430 140276907915072 estimator.py:1145] Calling model_fn.\n",
      "W1107 15:38:37.659662 140276907915072 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1107 15:38:38.160088 140276907915072 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/feature_column/feature_column_v2.py:4207: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "W1107 15:38:38.161123 140276907915072 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/feature_column/feature_column_v2.py:4262: IdentityCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "W1107 15:38:38.770661 140276907915072 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/canned/head.py:437: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W1107 15:38:38.923841 140276907915072 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "I1107 15:38:38.958616 140276907915072 estimator.py:1147] Done calling model_fn.\n",
      "I1107 15:38:38.959794 140276907915072 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
      "I1107 15:38:39.183840 140276907915072 monitored_session.py:240] Graph was finalized.\n",
      "I1107 15:38:39.501005 140276907915072 session_manager.py:500] Running local_init_op.\n",
      "I1107 15:38:39.524097 140276907915072 session_manager.py:502] Done running local_init_op.\n",
      "I1107 15:38:39.962077 140276907915072 basic_session_run_hooks.py:606] Saving checkpoints for 0 into data/training/model.ckpt.\n",
      "I1107 15:38:40.718049 140276907915072 basic_session_run_hooks.py:262] loss = 21.786713, step = 1\n",
      "I1107 15:38:41.037869 140276907915072 basic_session_run_hooks.py:606] Saving checkpoints for 3 into data/training/model.ckpt.\n",
      "I1107 15:38:41.256871 140276907915072 estimator.py:368] Loss for final step: 20.69631.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.canned.dnn.DNNClassifier at 0x7f941e825b70>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.train(input_fn=lambda: training_input_fn(\n",
    "                                    tf_transform_output, \n",
    "                                    os.path.join(trns_output, 'train' + '*'),\n",
    "                                    BATCH_SIZE, \n",
    "                                    \"tips\"), \n",
    "                steps=STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "block:eval",
     "prev:train"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1107 15:38:46.951269 140276907915072 estimator.py:1145] Calling model_fn.\n",
      "W1107 15:38:47.779766 140276907915072 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/metrics_impl.py:2027: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "W1107 15:38:48.069793 140276907915072 metrics_impl.py:804] Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "W1107 15:38:48.098131 140276907915072 metrics_impl.py:804] Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "I1107 15:38:48.312572 140276907915072 estimator.py:1147] Done calling model_fn.\n",
      "I1107 15:38:48.338888 140276907915072 evaluation.py:255] Starting evaluation at 2019-11-07T15:38:48Z\n",
      "I1107 15:38:48.440474 140276907915072 monitored_session.py:240] Graph was finalized.\n",
      "W1107 15:38:48.441548 140276907915072 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "I1107 15:38:48.443932 140276907915072 saver.py:1280] Restoring parameters from data/training/model.ckpt-3\n",
      "I1107 15:38:48.548900 140276907915072 session_manager.py:500] Running local_init_op.\n",
      "I1107 15:38:48.596199 140276907915072 session_manager.py:502] Done running local_init_op.\n",
      "I1107 15:38:49.062592 140276907915072 evaluation.py:167] Evaluation [5/50]\n",
      "I1107 15:38:49.146057 140276907915072 evaluation.py:167] Evaluation [10/50]\n",
      "I1107 15:38:49.237446 140276907915072 evaluation.py:167] Evaluation [15/50]\n",
      "I1107 15:38:49.342117 140276907915072 evaluation.py:167] Evaluation [20/50]\n",
      "I1107 15:38:49.405292 140276907915072 evaluation.py:167] Evaluation [25/50]\n",
      "I1107 15:38:49.478563 140276907915072 evaluation.py:167] Evaluation [30/50]\n",
      "I1107 15:38:49.547398 140276907915072 evaluation.py:167] Evaluation [35/50]\n",
      "I1107 15:38:49.635361 140276907915072 evaluation.py:167] Evaluation [40/50]\n",
      "I1107 15:38:49.688598 140276907915072 evaluation.py:167] Evaluation [45/50]\n",
      "I1107 15:38:49.747307 140276907915072 evaluation.py:167] Evaluation [50/50]\n",
      "I1107 15:38:49.863059 140276907915072 evaluation.py:275] Finished evaluation at 2019-11-07-15:38:49\n",
      "I1107 15:38:49.867191 140276907915072 estimator.py:2039] Saving dict for global step 3: accuracy = 0.769375, accuracy_baseline = 0.758125, auc = 0.8707819, auc_precision_recall = 0.600909, average_loss = 0.55728555, global_step = 3, label/mean = 0.241875, loss = 17.833138, precision = 0.61538464, prediction/mean = 0.40588272, recall = 0.12403101\n",
      "I1107 15:38:50.037165 140276907915072 estimator.py:2099] Saving 'checkpoint_path' summary for global step 3: data/training/model.ckpt-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.769375, 'accuracy_baseline': 0.758125, 'auc': 0.8707819, 'auc_precision_recall': 0.600909, 'average_loss': 0.55728555, 'label/mean': 0.241875, 'loss': 17.833138, 'precision': 0.61538464, 'prediction/mean': 0.40588272, 'recall': 0.12403101, 'global_step': 3}\n"
     ]
    }
   ],
   "source": [
    "eval_result = estimator.evaluate(input_fn=lambda: training_input_fn(\n",
    "                                                    tf_transform_output, \n",
    "                                                    os.path.join(trns_output, 'eval' + '*'),\n",
    "                                                    BATCH_SIZE, \n",
    "                                                    \"tips\"), \n",
    "                                 steps=50)\n",
    "\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF Model Analysis docs: https://www.tensorflow.org/tfx/model_analysis/get_started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true,
    "tags": [
     "skip"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1107 15:40:23.048025 140276907915072 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_1:0\\022\\022vocab_payment_type\"\n",
      "\n",
      "W1107 15:40:23.053156 140276907915072 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_3:0\\022\\031vocab_pickup_census_tract\"\n",
      "\n",
      "W1107 15:40:23.054471 140276907915072 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_4:0\\022\\rvocab_company\"\n",
      "\n",
      "W1107 15:40:23.056294 140276907915072 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_7:0\\022\\nvocab_tips\"\n",
      "\n",
      "W1107 15:40:23.056940 140276907915072 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_8:0\\022\\033vocab_pickup_community_area\"\n",
      "\n",
      "W1107 15:40:23.057396 140276907915072 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_9:0\\022\\032vocab_dropoff_census_tract\"\n",
      "\n",
      "W1107 15:40:23.057962 140276907915072 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\014\\n\\nConst_13:0\\022\\034vocab_dropoff_community_area\"\n",
      "\n",
      "I1107 15:40:23.059064 140276907915072 saver.py:1499] Saver not created because there are no variables in the graph to restore\n",
      "I1107 15:40:23.082942 140276907915072 estimator.py:1145] Calling model_fn.\n",
      "W1107 15:40:24.297939 140276907915072 metrics_impl.py:804] Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "W1107 15:40:24.317515 140276907915072 metrics_impl.py:804] Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "I1107 15:40:24.339512 140276907915072 estimator.py:1147] Done calling model_fn.\n",
      "I1107 15:40:24.349347 140276907915072 export_utils.py:170] Signatures INCLUDED in export for Classify: None\n",
      "I1107 15:40:24.352788 140276907915072 export_utils.py:170] Signatures INCLUDED in export for Regress: None\n",
      "I1107 15:40:24.354598 140276907915072 export_utils.py:170] Signatures INCLUDED in export for Predict: None\n",
      "I1107 15:40:24.355431 140276907915072 export_utils.py:170] Signatures INCLUDED in export for Train: None\n",
      "I1107 15:40:24.357363 140276907915072 export_utils.py:170] Signatures INCLUDED in export for Eval: ['eval']\n",
      "W1107 15:40:24.358222 140276907915072 export_utils.py:182] Export includes no default signature!\n",
      "I1107 15:40:24.402934 140276907915072 saver.py:1280] Restoring parameters from data/training/model.ckpt-3\n",
      "I1107 15:40:24.491941 140276907915072 builder_impl.py:661] Assets added to graph.\n",
      "I1107 15:40:24.494876 140276907915072 builder_impl.py:770] Assets written to: data/training/tfma_eval_model_dir/temp-b'1573141222'/assets\n",
      "I1107 15:40:24.718292 140276907915072 builder_impl.py:421] SavedModel written to: data/training/tfma_eval_model_dir/temp-b'1573141222'/saved_model.pb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'data/training/tfma_eval_model_dir/1573141222'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement model load and params analysis\n",
    "\n",
    "def eval_input_receiver_fn(transformed_output):\n",
    "    \"\"\"Build everything needed for the tf-model-analysis to run the model.\n",
    "    Args:\n",
    "      transformed_output: tft.TFTransformOutput\n",
    "    Returns:\n",
    "      EvalInputReceiver function, which contains:\n",
    "        - Tensorflow graph which parses raw untranformed features, applies the\n",
    "          tf-transform preprocessing operators.\n",
    "        - Set of raw, untransformed features.\n",
    "        - Label against which predictions will be compared.\n",
    "    \"\"\"\n",
    "    serialized_tf_example = tf.compat.v1.placeholder(\n",
    "        dtype=tf.string, shape=[None], name='input_example_tensor')\n",
    "    features = tf.io.parse_example(serialized_tf_example, transformed_output.raw_feature_spec())\n",
    "    transformed_features = transformed_output.transform_raw_features(features)\n",
    "    receiver_tensors = {'examples': serialized_tf_example}\n",
    "    return tfma.export.EvalInputReceiver(\n",
    "        features=transformed_features,\n",
    "        receiver_tensors=receiver_tensors,\n",
    "        labels=transformed_features[LABEL_KEY])\n",
    "\n",
    "# EXPORT MODEL\n",
    "eval_model_dir = os.path.join(training_output, 'tfma_eval_model_dir')\n",
    "tfma.export.export_eval_savedmodel(\n",
    "    estimator=estimator,\n",
    "    export_dir_base=eval_model_dir,\n",
    "    eval_input_receiver_fn=(lambda: eval_input_receiver_fn(tf_transform_output)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kubeflow_notebook": {
   "docker_image": "docker.io/stefanofioravanzo/kale-notebook:0.9",
   "experiment": {
    "id": "new",
    "name": "Taxicab"
   },
   "experiment_name": "Taxicab",
   "pipeline_description": "Use TFX components and Apache Beam to run a ML job over the Chicago taxicab dataset",
   "pipeline_name": "taxicab",
   "volumes": [
    {
     "annotations": [],
     "mount_point": "/home/jovyan",
     "name": "workspace-kale-dt49raygc",
     "size": 6,
     "size_type": "Gi",
     "snapshot": false,
     "type": "clone"
    }
   ]
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
