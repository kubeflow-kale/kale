"""
Generated Kubeflow Pipelines v2 DSL from Annotated Jupyter Notebook
Auto-generated code - modify as needed for your specific requirements

Generated by: Pipeline Builder Extension
Source: Annotated Jupyter Notebook
Target: KFP v2 DSL Python
"""

from kfp import dsl, compiler
from kfp.dsl import Input, Output, Model, Dataset, Metrics, Artifact, component, pipeline
from typing import NamedTuple, Dict, List, Any
import os

# =============================================================================
# GENERATED COMPONENTS FROM NOTEBOOK CELLS
# =============================================================================

@dsl.component(
    base_image='python:3.9',
    packages_to_install=['matplotlib', 'numpy', 'pandas', 'scikit-learn']
)
def data_loading() -> NamedTuple('Outputs', [('dataset', Dataset), ('metrics', Metrics), ('artifacts', Artifact)]):
    """
    Pipeline step: step:data-loading
    Generated from notebook cell 4
    Original tags: step:data-loading
    """
    # Imports
    import os
    import sys
    import json
    import pickle
    from pathlib import Path
    
    print(f"ğŸ”„ Executing data_loading component...")
    print(f"ğŸ“ Original cell content from notebook cell 4")
    
    # Original cell code
    # Data Loading and Initial Exploration
    import pandas as pd
    import numpy as np
    from sklearn.datasets import load_iris
    import matplotlib.pyplot as plt

    # Load the iris dataset
    print("ğŸ”„ Loading iris dataset...")
    iris = load_iris()

    # Create DataFrame
    df = pd.DataFrame(iris.data, columns=iris.feature_names)
    df['target'] = iris.target
    df['target_name'] = df['target'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})

    print(f"âœ… Dataset loaded successfully!")
    print(f"ğŸ“Š Dataset shape: {df.shape}")
    print(f"ğŸ·ï¸ Features: {list(df.columns[:-2])}")
    print(f"ğŸ¯ Target classes: {df['target_name'].unique()}")

    # Basic statistics
    print("\nğŸ“ˆ Dataset Overview:")
    print(df.describe())

    # Save some key metrics
    total_samples = len(df)
    num_features = len(iris.feature_names)
    num_classes = len(iris.target_names)

    print(f"\nğŸ“‹ Summary: {total_samples} samples, {num_features} features, {num_classes} classes")
    
    print(f"âœ… data_loading component execution completed")
    
    # Return results
    from collections import namedtuple
    Outputs = namedtuple('Outputs', ['dataset', 'metrics', 'artifacts'])
    return Outputs(locals().get("dataset", f"Missing: dataset"), locals().get("metrics", f"Missing: metrics"), locals().get("artifacts", f"Missing: artifacts"))

@dsl.component(
    base_image='python:3.9',
    packages_to_install=['numpy', 'pandas', 'scikit-learn']
)
def preprocessing() -> str:
    """
    Pipeline step: step:preprocessing
    Generated from notebook cell 5
    Original tags: step:preprocessing
    """
    # Imports
    import os
    import sys
    import json
    import pickle
    from pathlib import Path
    
    print(f"ğŸ”„ Executing preprocessing component...")
    print(f"ğŸ“ Original cell content from notebook cell 5")
    
    # Original cell code
    # Data Preprocessing and Feature Engineering
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import StandardScaler
    from sklearn.preprocessing import LabelEncoder

    print("ğŸ”„ Starting data preprocessing...")

    # Prepare features and target
    X = df.drop(['target', 'target_name'], axis=1)
    y = df['target']

    print(f"ğŸ“Š Features shape: {X.shape}")
    print(f"ğŸ¯ Target shape: {y.shape}")

    # Split the data
    test_size = 0.2
    random_state = 42

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state, stratify=y
    )

    print(f"ğŸ“ˆ Training set: {X_train.shape[0]} samples")
    print(f"ğŸ“‰ Test set: {X_test.shape[0]} samples")

    # Feature scaling
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    print("ğŸ”§ Feature scaling completed")

    # Check for missing values
    missing_values = df.isnull().sum().sum()
    print(f"ğŸ” Missing values: {missing_values}")

    # Feature statistics after scaling
    train_mean = np.mean(X_train_scaled, axis=0)
    train_std = np.std(X_train_scaled, axis=0)

    print("âœ… Preprocessing completed successfully!")
    print(f"ğŸ“Š Scaled features - Mean: {train_mean.round(3)}")
    print(f"ğŸ“Š Scaled features - Std: {train_std.round(3)}")

    preprocessing_summary = {
        'train_samples': len(X_train),
        'test_samples': len(X_test),
        'features': X_train.shape[1],
        'test_size_ratio': test_size
    }
    
    print(f"âœ… preprocessing component execution completed")
    
    # Return results
    # Try to return meaningful output from the executed code
    result_vars = [k for k in locals().keys() if not k.startswith('_') and not callable(locals()[k])]
    if result_vars:
        # Return a summary of what was created/computed
        summary = f"Executed successfully. Created variables: {', '.join(result_vars[:5])}"
        if len(result_vars) > 5:
            summary += f" and {len(result_vars) - 5} more"
        return summary
    else:
        return "Step completed successfully"

@dsl.component(
    base_image='python:3.9',
    packages_to_install=['numpy', 'pandas', 'scikit-learn']
)
def training() -> NamedTuple('Outputs', [('model', Model), ('metrics', Metrics)]):
    """
    Pipeline step: step:training
    Generated from notebook cell 6
    Original tags: step:training
    """
    # Imports
    import os
    import sys
    import json
    import pickle
    from pathlib import Path
    
    print(f"ğŸ”„ Executing training component...")
    print(f"ğŸ“ Original cell content from notebook cell 6")
    
    # Original cell code
    # Model Training
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.svm import SVC
    from sklearn.metrics import accuracy_score
    import time

    print("ğŸ”„ Starting model training...")

    # Define model parameters
    rf_params = {
        'n_estimators': 100,
        'random_state': 42,
        'max_depth': 5
    }

    lr_params = {
        'random_state': 42,
        'max_iter': 1000
    }

    svm_params = {
        'random_state': 42,
        'kernel': 'rbf'
    }

    # Train multiple models
    models = {}
    training_times = {}

    print("ğŸŒ² Training Random Forest...")
    start_time = time.time()
    rf_model = RandomForestClassifier(**rf_params)
    rf_model.fit(X_train_scaled, y_train)
    training_times['RandomForest'] = time.time() - start_time
    models['RandomForest'] = rf_model
    print(f"   â±ï¸ Training time: {training_times['RandomForest']:.3f}s")

    print("ğŸ“Š Training Logistic Regression...")
    start_time = time.time()
    lr_model = LogisticRegression(**lr_params)
    lr_model.fit(X_train_scaled, y_train)
    training_times['LogisticRegression'] = time.time() - start_time
    models['LogisticRegression'] = lr_model
    print(f"   â±ï¸ Training time: {training_times['LogisticRegression']:.3f}s")

    print("ğŸ¯ Training SVM...")
    start_time = time.time()
    svm_model = SVC(**svm_params)
    svm_model.fit(X_train_scaled, y_train)
    training_times['SVM'] = time.time() - start_time
    models['SVM'] = svm_model
    print(f"   â±ï¸ Training time: {training_times['SVM']:.3f}s")

    print("âœ… All models trained successfully!")
    print(f"ğŸ”§ Trained {len(models)} models: {list(models.keys())}")

    # Quick training accuracy check
    train_accuracies = {}
    for name, model in models.items():
        train_pred = model.predict(X_train_scaled)
        train_acc = accuracy_score(y_train, train_pred)
        train_accuracies[name] = train_acc
        print(f"ğŸ“ˆ {name} training accuracy: {train_acc:.4f}")

    best_train_model = max(train_accuracies, key=train_accuracies.get)
    print(f"ğŸ† Best training accuracy: {best_train_model} ({train_accuracies[best_train_model]:.4f})")
    
    print(f"âœ… training component execution completed")
    
    # Return results
    from collections import namedtuple
    Outputs = namedtuple('Outputs', ['model', 'metrics'])
    return Outputs(locals().get("model", f"Missing: model"), locals().get("metrics", f"Missing: metrics"))

@dsl.component(
    base_image='python:3.9',
    packages_to_install=['numpy', 'pandas', 'scikit-learn']
)
def evaluation() -> NamedTuple('Outputs', [('model', Model), ('metrics', Metrics)]):
    """
    Pipeline step: step:evaluation
    Generated from notebook cell 7
    Original tags: step:evaluation
    """
    # Imports
    import os
    import sys
    import json
    import pickle
    from pathlib import Path
    
    print(f"ğŸ”„ Executing evaluation component...")
    print(f"ğŸ“ Original cell content from notebook cell 7")
    
    # Original cell code
    # Model Evaluation and Performance Analysis
    from sklearn.metrics import classification_report, confusion_matrix
    from sklearn.metrics import precision_score, recall_score, f1_score
    import numpy as np

    print("ğŸ”„ Starting model evaluation...")

    # Evaluate all models
    evaluation_results = {}

    for name, model in models.items():
        print(f"\nğŸ“Š Evaluating {name}...")

        # Make predictions
        y_pred = model.predict(X_test_scaled)

        # Calculate metrics
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average='weighted')
        recall = recall_score(y_test, y_pred, average='weighted')
        f1 = f1_score(y_test, y_pred, average='weighted')

        # Store results
        evaluation_results[name] = {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'training_time': training_times[name]
        }

        print(f"   ğŸ¯ Accuracy: {accuracy:.4f}")
        print(f"   ğŸ¯ Precision: {precision:.4f}")
        print(f"   ğŸ¯ Recall: {recall:.4f}")
        print(f"   ğŸ¯ F1-Score: {f1:.4f}")

    # Find best model
    best_model_name = max(evaluation_results, key=lambda x: evaluation_results[x]['accuracy'])
    best_model = models[best_model_name]
    best_accuracy = evaluation_results[best_model_name]['accuracy']

    print(f"\nğŸ† Best Model: {best_model_name}")
    print(f"ğŸ† Best Accuracy: {best_accuracy:.4f}")

    # Detailed evaluation of best model
    print(f"\nğŸ“‹ Detailed Classification Report for {best_model_name}:")
    y_pred_best = best_model.predict(X_test_scaled)
    print(classification_report(y_test, y_pred_best, target_names=iris.target_names))

    # Confusion matrix
    conf_matrix = confusion_matrix(y_test, y_pred_best)
    print(f"\nğŸ” Confusion Matrix for {best_model_name}:")
    print(conf_matrix)

    # Model comparison summary
    print(f"\nğŸ“Š Model Comparison Summary:")
    for name, results in evaluation_results.items():
        print(f"{name:15} | Acc: {results['accuracy']:.4f} | Time: {results['training_time']:.3f}s")

    # Final metrics for pipeline output
    final_accuracy = best_accuracy
    final_model_name = best_model_name
    total_models_trained = len(models)
    
    print(f"âœ… evaluation component execution completed")
    
    # Return results
    from collections import namedtuple
    Outputs = namedtuple('Outputs', ['model', 'metrics'])
    return Outputs(locals().get("model", f"Missing: model"), locals().get("metrics", f"Missing: metrics"))

@dsl.component(
    base_image='python:3.9',
    packages_to_install=['numpy', 'pandas', 'scikit-learn']
)
def pipeline_parameters_8() -> dict:
    """
    Pipeline parameters configuration
    Generated from notebook cell 8
    """
    import json
    
    print(f"ğŸ”„ Loading pipeline parameters from cell 8...")
    
    # Pipeline Configuration Parameters
    print("ğŸ”„ Setting up pipeline parameters...")

    # Data parameters
    dataset_name = "iris"
    target_column = "target"
    test_size_param = 0.2
    random_state_param = 42

    # Model parameters
    rf_n_estimators = 100
    rf_max_depth = 5
    lr_max_iter = 1000
    svm_kernel = "rbf"

    # Evaluation parameters
    scoring_metric = "accuracy"
    cv_folds = 5

    # Pipeline metadata
    pipeline_version = "1.0.0"
    pipeline_description = "Iris classification pipeline with multiple models"
    author = "Pipeline Builder Extension"

    print("âœ… Pipeline parameters configured:")
    print(f"   ğŸ“Š Dataset: {dataset_name}")
    print(f"   ğŸ¯ Target: {target_column}")
    print(f"   ğŸ“ˆ Test size: {test_size_param}")
    print(f"   ğŸ”¢ Random state: {random_state_param}")
    print(f"   ğŸ“‹ Version: {pipeline_version}")
    
    # Extract parameters from local variables
    params = {k: v for k, v in locals().items() 
             if not k.startswith('_') and not callable(v) and k not in ['json', 'print']}
    
    print(f"ğŸ“‹ Loaded parameters: {list(params.keys())}")
    print("âœ… Parameters loaded successfully")
    
    return params

@dsl.component(
    base_image='python:3.9',
    packages_to_install=['numpy', 'pandas', 'scikit-learn']
)
def pipeline_metrics_9() -> NamedTuple('Metrics', [('metrics', dict)]):
    """
    Pipeline metrics calculation
    Generated from notebook cell 9
    """
    import json
    from typing import NamedTuple
    
    print(f"ğŸ”„ Computing metrics from cell 9...")
    
    # Pipeline Results and Metrics Summary
    print("ğŸ”„ Generating pipeline metrics summary...")

    # Calculate overall pipeline metrics
    pipeline_success = True
    total_execution_time = sum(training_times.values())

    # Performance metrics
    avg_accuracy = np.mean([results['accuracy'] for results in evaluation_results.values()])
    std_accuracy = np.std([results['accuracy'] for results in evaluation_results.values()])

    # Model diversity
    accuracy_range = max(evaluation_results.values(), key=lambda x: x['accuracy'])['accuracy'] - \
                    min(evaluation_results.values(), key=lambda x: x['accuracy'])['accuracy']

    # Final pipeline metrics
    pipeline_metrics = {
        'pipeline_success': pipeline_success,
        'best_model': best_model_name,
        'best_accuracy': float(best_accuracy),
        'average_accuracy': float(avg_accuracy),
        'accuracy_std': float(std_accuracy),
        'accuracy_range': float(accuracy_range),
        'total_models': total_models_trained,
        'total_training_time': float(total_execution_time),
        'dataset_size': total_samples,
        'test_samples': len(X_test),
        'num_features': num_features,
        'num_classes': num_classes
    }

    print("âœ… Pipeline metrics computed:")
    print(f"   ğŸ† Best model: {best_model_name} ({best_accuracy:.4f})")
    print(f"   ğŸ“Š Average accuracy: {avg_accuracy:.4f} Â± {std_accuracy:.4f}")
    print(f"   â±ï¸ Total training time: {total_execution_time:.3f}s")
    print(f"   ğŸ”¢ Models trained: {total_models_trained}")
    print(f"   ğŸ“ˆ Accuracy range: {accuracy_range:.4f}")

    # Success criteria
    success_threshold = 0.90
    pipeline_passed = best_accuracy >= success_threshold

    print(f"\nğŸ¯ Pipeline Quality Assessment:")
    print(f"   Success threshold: {success_threshold}")
    print(f"   Pipeline passed: {'âœ… YES' if pipeline_passed else 'âŒ NO'}")

    if pipeline_passed:
        print("ğŸ‰ Pipeline completed successfully with high accuracy!")
    else:
        print("âš ï¸ Pipeline completed but accuracy below threshold")
    
    # Extract metrics from local variables
    metrics = {k: v for k, v in locals().items() 
              if not k.startswith('_') and not callable(v) and 
              isinstance(v, (int, float, str, bool)) and 
              k not in ['json', 'NamedTuple', 'print']}
    
    print(f"ğŸ“Š Computed metrics: {list(metrics.keys())}")
    print("âœ… Metrics computation completed")
    
    from collections import namedtuple
    Metrics = namedtuple('Metrics', ['metrics'])
    return Metrics(metrics)

# =============================================================================
# PIPELINE DEFINITION
# =============================================================================

@dsl.pipeline(
    name='iris_example',
    description='Generated from annotated notebook'
)
def iris_example_pipeline():
    """
    Generated pipeline from annotated notebook cells
    
    Pipeline steps:
    - Step 1: data_loading
    - Step 2: preprocessing
    - Step 3: training
    - Step 4: evaluation
    - Step 5: pipeline_parameters_8
    - Step 6: pipeline_metrics_9
    """
    data_loading_task = data_loading()
    preprocessing_task = preprocessing().after(data_loading_task)
    training_task = training().after(preprocessing_task)
    evaluation_task = evaluation().after(training_task)
    pipeline_parameters_8_task = pipeline_parameters_8().after(evaluation_task)
    pipeline_metrics_9_task = pipeline_metrics_9().after(pipeline_parameters_8_task)

# =============================================================================
# COMPILATION AND EXECUTION
# =============================================================================

def compile_pipeline(output_path: str = None) -> str:
    """
    Compile the pipeline to a YAML file
    
    Args:
        output_path: Path where to save the compiled pipeline
        
    Returns:
        Path to the compiled pipeline file
    """
    if output_path is None:
        output_path = 'iris_example_kfp_pipeline.yaml'
    
    compiler.Compiler().compile(
        pipeline_func=iris_example_pipeline,
        package_path=output_path
    )
    
    print(f"âœ… Pipeline compiled successfully!")
    print(f"ğŸ“„ Generated file: " + output_path)
    return output_path


def run_pipeline_local():
    """
    Run the pipeline locally for testing
    """
    print("ğŸš€ Running pipeline locally...")
    try:
        iris_example_pipeline()
        print("âœ… Local pipeline execution completed!")
    except Exception as e:
        print(f"âŒ Local pipeline execution failed: " + str(e))


def submit_pipeline(host: str = None, **pipeline_args) -> str:
    """
    Submit the pipeline to a KFP cluster
    
    Args:
        host: KFP cluster host URL
        **pipeline_args: Arguments to pass to the pipeline
        
    Returns:
        Run ID of the submitted pipeline
    """
    if host is None:
        raise ValueError("KFP host URL is required to submit the pipeline")
    
    import kfp
    
    try:
        # Connect to KFP cluster
        client = kfp.Client(host=host)
        
        # Filter out KFP-specific arguments that aren't pipeline parameters
        kfp_args = {'experiment_name', 'run_name', 'namespace'}
        actual_pipeline_args = {}
        for key, value in pipeline_args.items():
            if key not in kfp_args:
                actual_pipeline_args[key] = value
        
        # Extract KFP-specific arguments
        experiment_name = pipeline_args.get('experiment_name', 'default')
        run_name = pipeline_args.get('run_name', 'iris_example-run')
        
        # Create or get experiment
        try:
            experiment = client.get_experiment(experiment_name=experiment_name)
            print("ğŸ“‹ Using existing experiment: " + experiment_name)
        except:
            try:
                experiment = client.create_experiment(name=experiment_name)
                print("ğŸ“‹ Created new experiment: " + experiment_name)
            except:
                print("âš ï¸ Using default experiment")
                experiment_name = None
        
        # Submit pipeline run with only actual pipeline parameters
        if experiment_name:
            run_result = client.create_run_from_pipeline_func(
                pipeline_func=iris_example_pipeline,
                arguments=actual_pipeline_args,
                run_name=run_name,
                experiment_name=experiment_name
            )
        else:
            run_result = client.create_run_from_pipeline_func(
                pipeline_func=iris_example_pipeline,
                arguments=actual_pipeline_args,
                run_name=run_name
            )
        
        print("ğŸš€ Pipeline submitted successfully!")
        print("ğŸ“‹ Run ID: " + run_result.run_id)
        print("ğŸ”— View run: " + host + "/#/runs/details/" + run_result.run_id)
        
        return run_result.run_id
        
    except Exception as e:
        print("âŒ Pipeline submission failed: " + str(e))
        raise


def main():
    """Main execution function"""
    import argparse
    import sys
    
    parser = argparse.ArgumentParser(description="Generated KFP Pipeline")
    parser.add_argument("--compile", action="store_true", default=True, 
                       help="Compile pipeline to YAML (default)")
    parser.add_argument("--run-local", action="store_true", 
                       help="Run pipeline locally for testing")
    parser.add_argument("--kfp-host", type=str, 
                       help="KFP cluster host URL for submission")
    parser.add_argument("--experiment", type=str, default="default",
                       help="KFP experiment name")
    parser.add_argument("--run-name", type=str,
                       help="KFP run name")
    parser.add_argument("--output", "-o", type=str,
                       help="Output path for compiled YAML")
    
    # If no arguments provided, just compile by default
    if len(sys.argv) == 1:
        print("ğŸ”¨ Compiling pipeline (default action)...")
        compile_pipeline()
        return
    
    args = parser.parse_args()
    
    try:
        if args.compile and not args.run_local and not args.kfp_host:
            # Default: compile to YAML
            print("ğŸ”¨ Compiling pipeline to YAML...")
            output_file = compile_pipeline(args.output)
            print("âœ… Pipeline compiled to: " + output_file)
            print("ğŸ’¡ Next steps:")
            print("   - Test locally: python " + sys.argv[0] + " --run-local")
            print("   - Submit to KFP: python " + sys.argv[0] + " --kfp-host <URL>")
        
        if args.run_local:
            print("ğŸ§ª Running pipeline locally...")
            run_pipeline_local()
        
        if args.kfp_host:
            print("ğŸš€ Submitting pipeline to KFP cluster: " + args.kfp_host)
            
            # Prepare submission arguments
            submission_args = {}
            if args.experiment:
                submission_args['experiment_name'] = args.experiment
            if args.run_name:
                submission_args['run_name'] = args.run_name
            
            run_id = submit_pipeline(
                host=args.kfp_host,
                **submission_args
            )
            print("âœ… Pipeline submitted with run ID: " + run_id)
    
    except Exception as e:
        print("âŒ Error: " + str(e))
        sys.exit(1)


if __name__ == "__main__":
    main()