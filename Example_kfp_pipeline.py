"""
Generated Kubeflow Pipelines v2 DSL from Annotated Jupyter Notebook
Auto-generated code - modify as needed for your specific requirements

Generated by: Pipeline Builder Extension
Source: Annotated Jupyter Notebook
Target: KFP v2 DSL Python
"""

from kfp import dsl, compiler
from kfp.dsl import Input, Output, Model, Dataset, Metrics, Artifact, component, pipeline
from typing import NamedTuple, Dict, List, Any
import os

# =============================================================================
# GENERATED COMPONENTS FROM NOTEBOOK CELLS
# =============================================================================

@dsl.component(
    base_image='python:3.9',
    packages_to_install=['numpy', 'pandas', 'scikit-learn']
)
def data_loading() -> Dataset:
    """
    Pipeline step: step:data-loading
    Generated from notebook cell 0
    Original tags: step:data-loading
    """
    # Imports
    import os
    import sys
    import json
    import pickle
    from pathlib import Path
    
    print(f"Executing data_loading component...")
    print(f"Original cell content from notebook cell 0")
    
    # Original cell code
    import pandas as pd
    import numpy as np

    print(" Creating simple dataset...")

    # Create a simple synthetic dataset
    np.random.seed(42)
    n_samples = 100

    # Features: age, income
    age = np.random.randint(18, 65, n_samples)
    income = age * 1000 + np.random.normal(0, 5000, n_samples)

    # Target: can_buy_house (1 if income > 50000, 0 otherwise)
    target = (income > 50000).astype(int)

    # Create DataFrame
    data = pd.DataFrame({
        'age': age,
        'income': income,
        'can_buy_house': target
    })

    print(f" Dataset created with {len(data)} samples")
    print(f" Features: age, income")
    print(f" Target: can_buy_house")
    print("\nFirst 5 rows:")
    print(data.head())

    dataset_size = len(data)
    feature_count = 2
    
    print(f"data_loading component execution completed")
    
    # Return results
    # Return the specific output if available
    if "dataset" in locals():
        return str(dataset) if hasattr(dataset, '__str__') else f"Created dataset"
    else:
        return "Expected output 'dataset' not found, but step completed"

@dsl.component(
    base_image='python:3.9',
    packages_to_install=['numpy', 'pandas', 'scikit-learn']
)
def preprocessing() -> str:
    """
    Pipeline step: step:preprocessing
    Generated from notebook cell 1
    Original tags: step:preprocessing
    """
    # Imports
    import os
    import sys
    import json
    import pickle
    from pathlib import Path
    
    print(f"Executing preprocessing component...")
    print(f"Original cell content from notebook cell 1")
    
    # Original cell code
    # Data preparation
    from sklearn.model_selection import train_test_split

    print(" Preparing data for training...")

    # Separate features and target
    X = data[['age', 'income']]
    y = data['can_buy_house']

    print(f" Features shape: {X.shape}")
    print(f" Target shape: {y.shape}")

    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )

    print(f" Training samples: {len(X_train)}")
    print(f" Test samples: {len(X_test)}")

    # Basic statistics
    print(f"\n Training data stats:")
    print(f"   Average age: {X_train['age'].mean():.1f}")
    print(f"   Average income: ${X_train['income'].mean():.0f}")
    print(f"   Positive cases: {y_train.sum()}/{len(y_train)}")

    train_samples = len(X_train)
    test_samples = len(X_test)

    print(" Data preparation completed!")
    
    print(f"preprocessing component execution completed")
    
    # Return results
    # Try to return meaningful output from the executed code
    result_vars = [k for k in locals().keys() if not k.startswith('_') and not callable(locals()[k])]
    if result_vars:
        # Return a summary of what was created/computed
        summary = f"Executed successfully. Created variables: {', '.join(result_vars[:5])}"
        if len(result_vars) > 5:
            summary += f" and {len(result_vars) - 5} more"
        return summary
    else:
        return "Step completed successfully"

@dsl.component(
    base_image='python:3.9',
    packages_to_install=['numpy', 'pandas', 'scikit-learn']
)
def training() -> NamedTuple('Outputs', [('model', Model), ('metrics', Metrics)]):
    """
    Pipeline step: step:training
    Generated from notebook cell 2
    Original tags: step:training
    """
    # Imports
    import os
    import sys
    import json
    import pickle
    from pathlib import Path
    
    print(f"Executing training component...")
    print(f"Original cell content from notebook cell 2")
    
    # Original cell code
    # Model training
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import accuracy_score

    print(" Training model...")

    # Create and train model
    model = LogisticRegression(random_state=42)
    model.fit(X_train, y_train)

    print(" Model training completed!")

    # Check training accuracy
    train_predictions = model.predict(X_train)
    train_accuracy = accuracy_score(y_train, train_predictions)

    print(f" Training accuracy: {train_accuracy:.3f}")

    # Model info
    model_type = "LogisticRegression"
    coefficients = model.coef_[0]

    print(f" Model coefficients:")
    print(f"   Age coefficient: {coefficients[0]:.4f}")
    print(f"   Income coefficient: {coefficients[1]:.6f}")

    training_accuracy = train_accuracy
    
    print(f"training component execution completed")
    
    # Return results
    from collections import namedtuple
    Outputs = namedtuple('Outputs', ['model', 'metrics'])
    return Outputs(locals().get("model", f"Missing: model"), locals().get("metrics", f"Missing: metrics"))

@dsl.component(
    base_image='python:3.9',
    packages_to_install=['numpy', 'pandas', 'scikit-learn']
)
def evaluation() -> Metrics:
    """
    Pipeline step: step:evaluation
    Generated from notebook cell 3
    Original tags: step:evaluation
    """
    # Imports
    import os
    import sys
    import json
    import pickle
    from pathlib import Path
    
    print(f"Executing evaluation component...")
    print(f"Original cell content from notebook cell 3")
    
    # Original cell code
    # Model evaluation
    from sklearn.metrics import classification_report

    print(" Evaluating model...")

    # Make predictions
    test_predictions = model.predict(X_test)
    test_accuracy = accuracy_score(y_test, test_predictions)

    print(f" Test accuracy: {test_accuracy:.3f}")

    # Detailed evaluation
    print("\n Classification Report:")
    print(classification_report(y_test, test_predictions))

    # Simple predictions on new data
    print("\n Sample predictions:")
    sample_data = [[25, 30000], [45, 80000], [35, 60000]]
    sample_predictions = model.predict(sample_data)

    for i, (age, income) in enumerate(sample_data):
        prediction = "Yes" if sample_predictions[i] == 1 else "No"
        print(f"   Age {age}, Income ${income:,} â†’ Can buy house: {prediction}")

    # Final metrics
    final_accuracy = test_accuracy
    total_correct = int(test_accuracy * len(y_test))
    model_performance = "Good" if test_accuracy > 0.8 else "Fair" if test_accuracy > 0.6 else "Poor"

    print(f"\n Evaluation completed!")
    print(f" Final accuracy: {final_accuracy:.3f}")
    print(f" Correct predictions: {total_correct}/{len(y_test)}")
    print(f" Model performance: {model_performance}")
    
    print(f"evaluation component execution completed")
    
    # Return results
    # Return the specific output if available
    if "metrics" in locals():
        return str(metrics) if hasattr(metrics, '__str__') else f"Created metrics"
    else:
        return "Expected output 'metrics' not found, but step completed"

# =============================================================================
# PIPELINE DEFINITION
# =============================================================================

@dsl.pipeline(
    name='Example',
    description='Generated from annotated notebook'
)
def Example_pipeline():
    """
    Generated pipeline from annotated notebook cells
    
    Pipeline steps:
    - Step 1: data_loading
    - Step 2: preprocessing
    - Step 3: training
    - Step 4: evaluation
    """
    data_loading_task = data_loading()
    preprocessing_task = preprocessing().after(data_loading_task)
    training_task = training().after(preprocessing_task)
    evaluation_task = evaluation().after(training_task)

# =============================================================================
# COMPILATION AND EXECUTION
# =============================================================================

def compile_pipeline(output_path: str = None) -> str:
    """
    Compile the pipeline to a YAML file
    
    Args:
        output_path: Path where to save the compiled pipeline
        
    Returns:
        Path to the compiled pipeline file
    """
    if output_path is None:
        output_path = 'Example_kfp_pipeline.yaml'
    
    compiler.Compiler().compile(
        pipeline_func=Example_pipeline,
        package_path=output_path
    )
    
    print(f" Pipeline compiled successfully!")
    print(f" Generated file: " + output_path)
    return output_path


def run_pipeline_local():
    """
    Run the pipeline locally for testing
    """
    print(" Running pipeline locally...")
    try:
        Example_pipeline()
        print(" Local pipeline execution completed!")
    except Exception as e:
        print(f" Local pipeline execution failed: " + str(e))
        
    except ImportError:
        print("  KFP local execution not available, running components individually...")
        try:
            # Fallback: run components individually without KFP
            print(" Running components individually...")
            
            # Get all component functions
            import inspect
            current_module = inspect.currentframe().f_globals
            
            # Find component functions (those decorated with @dsl.component)
            component_functions = []
            for name, obj in current_module.items():
                if (callable(obj) and 
                    hasattr(obj, '__name__') and 
                    not name.startswith('_') and
                    name not in ['main', 'compile_pipeline', 'run_pipeline_local', 'submit_pipeline']):
                    component_functions.append((name, obj))
            
            # Sort components to run in order
            component_functions.sort(key=lambda x: x[0])
            
            print(f" Found {len(component_functions)} components to execute")
            
            # Execute each component
            results = {}
            for comp_name, comp_func in component_functions:
                print(f"\n Executing {comp_name}...")
                try:
                    # Get the actual function (unwrap from KFP decorator if needed)
                    if hasattr(comp_func, 'python_func'):
                        actual_func = comp_func.python_func
                    else:
                        actual_func = comp_func
                    
                    # Execute the component
                    result = actual_func()
                    results[comp_name] = result
                    print(f" {comp_name} completed: {result}")
                    
                except Exception as e:
                    print(f" {comp_name} failed: {e}")
                    raise e
            
            print("\n All components executed successfully!")
            print(" Results summary:")
            for comp_name, result in results.items():
                result_str = str(result)[:100] + "..." if len(str(result)) > 100 else str(result)
                print(f"   {comp_name}: {result_str}")
                
        except Exception as e:
            print(f" Component execution failed: {e}")
            raise e
            
    except Exception as e:
        print(f" Local pipeline execution failed: {e}")
        raise e


def submit_pipeline(host: str = None, **pipeline_args) -> str:
    """
    Submit the pipeline to a KFP cluster
    
    Args:
        host: KFP cluster host URL
        **pipeline_args: Arguments to pass to the pipeline
        
    Returns:
        Run ID of the submitted pipeline
    """
    if host is None:
        raise ValueError("KFP host URL is required to submit the pipeline")
    
    import kfp
    
    try:
        # Connect to KFP cluster
        client = kfp.Client(host=host)
        
        # Filter out KFP-specific arguments that aren't pipeline parameters
        kfp_args = {'experiment_name', 'run_name', 'namespace'}
        actual_pipeline_args = {}
        for key, value in pipeline_args.items():
            if key not in kfp_args:
                actual_pipeline_args[key] = value
        
        # Extract KFP-specific arguments
        experiment_name = pipeline_args.get('experiment_name', 'default')
        run_name = pipeline_args.get('run_name', 'Example-run')
        
        # Create or get experiment
        try:
            experiment = client.get_experiment(experiment_name=experiment_name)
            print(" Using existing experiment: " + experiment_name)
        except:
            try:
                experiment = client.create_experiment(name=experiment_name)
                print(" Created new experiment: " + experiment_name)
            except:
                print(" Using default experiment")
                experiment_name = None
        
        # Submit pipeline run with only actual pipeline parameters
        if experiment_name:
            run_result = client.create_run_from_pipeline_func(
                pipeline_func=Example_pipeline,
                arguments=actual_pipeline_args,
                run_name=run_name,
                experiment_name=experiment_name
            )
        else:
            run_result = client.create_run_from_pipeline_func(
                pipeline_func=Example_pipeline,
                arguments=actual_pipeline_args,
                run_name=run_name
            )
        
        print("Pipeline submitted successfully!")
        print("Run ID: " + run_result.run_id)
        print("View run: " + host + "/#/runs/details/" + run_result.run_id)
        
        return run_result.run_id
        
    except Exception as e:
        print("Pipeline submission failed: " + str(e))
        raise


def main():
    """Main execution function"""
    import argparse
    import sys
    
    parser = argparse.ArgumentParser(description="Generated KFP Pipeline")
    parser.add_argument("--compile", action="store_true", default=True, 
                       help="Compile pipeline to YAML (default)")
    parser.add_argument("--run-local", action="store_true", 
                       help="Run pipeline locally for testing")
    parser.add_argument("--kfp-host", type=str, 
                       help="KFP cluster host URL for submission")
    parser.add_argument("--experiment", type=str, default="default",
                       help="KFP experiment name")
    parser.add_argument("--run-name", type=str,
                       help="KFP run name")
    parser.add_argument("--output", "-o", type=str,
                       help="Output path for compiled YAML")
    
    # If no arguments provided, just compile by default
    if len(sys.argv) == 1:
        print(" Compiling pipeline (default action)...")
        compile_pipeline()
        return
    
    args = parser.parse_args()
    
    try:
        if args.compile and not args.run_local and not args.kfp_host:
            # Default: compile to YAML
            print(" Compiling pipeline to YAML...")
            output_file = compile_pipeline(args.output)
            print(" Pipeline compiled to: " + output_file)
            print(" Next steps:")
            print("   - Test locally: python " + sys.argv[0] + " --run-local")
            print("   - Submit to KFP: python " + sys.argv[0] + " --kfp-host <URL>")
        
        if args.run_local:
            print(" Running pipeline locally...")
            run_pipeline_local()
        
        if args.kfp_host:
            print(" Submitting pipeline to KFP cluster: " + args.kfp_host)
            
            # Prepare submission arguments
            submission_args = {}
            if args.experiment:
                submission_args['experiment_name'] = args.experiment
            if args.run_name:
                submission_args['run_name'] = args.run_name
            
            run_id = submit_pipeline(
                host=args.kfp_host,
                **submission_args
            )
            print(" Pipeline submitted with run ID: " + run_id)
    
    except Exception as e:
        print(" Error: " + str(e))
        sys.exit(1)


if __name__ == "__main__":
    main()