import { ServiceManager } from '@jupyterlab/services';
import { Kernel, KernelMessage } from '@jupyterlab/services';

/**
 * Enhanced manager class for handling kernel communication and direct notebook to KFP conversion
 */
export class KernelManager {
  private _serviceManager: ServiceManager.IManager;
  private _kernel: Kernel.IKernelConnection | null = null;

  constructor(serviceManager: ServiceManager.IManager) {
    this._serviceManager = serviceManager;
  }

  /**
   * Start a new Python kernel
   */
  async startKernel(): Promise<void> {
    try {
      // Shutdown existing kernel if any
      if (this._kernel) {
        await this.shutdown();
      }

      // Start a new kernel
      this._kernel = await this._serviceManager.kernels.startNew({
        name: 'python3'
      });

      console.log('Kernel started:', this._kernel.id);
      console.log('Kernel is ready');

      // Set up status change handler
      this._kernel.statusChanged.connect((sender, status) => {
        console.log('Kernel status changed:', status);
      });

    } catch (error) {
      console.error('Failed to start kernel:', error);
      throw error;
    }
  }

  /**
   * Execute Python code in the kernel
   */
  async executeCode(code: string): Promise<string> {
    if (!this._kernel) {
      throw new Error('Kernel not started. Call startKernel() first.');
    }

    return new Promise((resolve, reject) => {
      let output = '';
      let errorOutput = '';

      const future = this._kernel!.requestExecute({
        code: code,
        store_history: false,
        allow_stdin: false
      });

      future.onIOPub = (msg: KernelMessage.IIOPubMessage) => {
        const msgType = msg.header.msg_type;
        
        switch (msgType) {
          case 'stream':
            const streamMsg = msg as KernelMessage.IStreamMsg;
            if (streamMsg.content.name === 'stdout') {
              output += streamMsg.content.text;
            } else if (streamMsg.content.name === 'stderr') {
              errorOutput += streamMsg.content.text;
            }
            break;
            
          case 'execute_result':
            const executeMsg = msg as KernelMessage.IExecuteResultMsg;
            if (executeMsg.content.data['text/plain']) {
              output += executeMsg.content.data['text/plain'] + '\n';
            }
            break;
            
          case 'display_data':
            const displayMsg = msg as KernelMessage.IDisplayDataMsg;
            if (displayMsg.content.data['text/plain']) {
              output += displayMsg.content.data['text/plain'] + '\n';
            }
            break;
            
          case 'error':
            const errorMsg = msg as KernelMessage.IErrorMsg;
            errorOutput += `${errorMsg.content.ename}: ${errorMsg.content.evalue}\n`;
            errorOutput += errorMsg.content.traceback.join('\n');
            break;
        }
      };

      future.onReply = (msg: KernelMessage.IExecuteReplyMsg) => {
        if (msg.content.status === 'ok') {
          resolve(output || 'Code executed successfully (no output)');
        } else {
          reject(new Error(errorOutput || 'Unknown execution error'));
        }
      };

      future.onStdin = (msg: KernelMessage.IStdinMessage) => {
        console.log('Stdin message received:', msg);
      };

      setTimeout(() => {
        reject(new Error('Execution timeout after 30 seconds'));
      }, 30000);
    });
  }

  /**
   * Get kernel information
   */
  async getKernelInfo(): Promise<any> {
    if (!this._kernel) {
      throw new Error('Kernel not started');
    }

    const future = this._kernel.requestKernelInfo();
    const reply = await future;
    
    if (!reply) {
      throw new Error('Failed to get kernel info - no reply received');
    }
    
    return reply.content;
  }

  /**
   * Check if kernel is alive
   */
  isAlive(): boolean {
    return this._kernel !== null && !this._kernel.isDisposed;
  }

  /**
   * Get kernel ID
   */
  getKernelId(): string | null {
    return this._kernel ? this._kernel.id : null;
  }

  /**
   * Interrupt the kernel
   */
  async interrupt(): Promise<void> {
    if (this._kernel) {
      await this._kernel.interrupt();
    }
  }

  /**
   * Restart the kernel
   */
  async restart(): Promise<void> {
    if (this._kernel) {
      await this._kernel.restart();
    }
  }

  /**
   * Shutdown the kernel
   */
  async shutdown(): Promise<void> {
    if (this._kernel) {
      try {
        await this._kernel.shutdown();
        this._kernel.dispose();
      } catch (error) {
        console.warn('Error shutting down kernel:', error);
      } finally {
        this._kernel = null;
      }
    }
  }

  /**
   * Enhanced initialization of the direct notebook to KFP converter
   */
  async initializeNotebookToKFPConverter(): Promise<string> {
    const code = `
import sys
import os
import subprocess

print("=== Enhanced Notebook to KFP Converter Initialization ===")

# Install required packages
required_packages = ['nbformat', 'kfp']
for package in required_packages:
    try:
        if package == 'kfp':
            import kfp
            print(f"âœ“ {package} already available (version: {kfp.__version__})")
        else:
            __import__(package)
            print(f"âœ“ {package} already available")
    except ImportError:
        print(f"ğŸ“¦ Installing {package}...")
        try:
            result = subprocess.run([sys.executable, '-m', 'pip', 'install', package], 
                                  capture_output=True, text=True, check=True)
            print(f"âœ… {package} installed successfully")
        except subprocess.CalledProcessError as e:
            print(f"âŒ Failed to install {package}: {e.stderr}")
            raise Exception(f"Package installation failed: {package}")

# Define converter paths to check
converter_paths = [
    os.path.join(os.getcwd(), 'src', 'processors', 'notebook_to_kfp_converter.py'),
    os.path.join(os.getcwd(), 'lib', 'processors', 'notebook_to_kfp_converter.py'),
    'notebook_to_kfp_converter.py'  # Current directory fallback
]

converter_loaded = False
loaded_from = None

print("\\nğŸ” Searching for converter file...")
for path in converter_paths:
    print(f"  ğŸ“ Checking: {path}")
    if os.path.exists(path):
        print(f"  âœ… Found converter at: {path}")
        try:
            # Check file size
            file_size = os.path.getsize(path)
            print(f"  ğŸ“Š File size: {file_size} bytes")
            
            if file_size == 0:
                print(f"  âš ï¸ File is empty: {path}")
                continue
            
            # Read the converter file content
            with open(path, 'r', encoding='utf-8') as f:
                converter_code = f.read()
            
            print(f"  ğŸ“„ Read {len(converter_code)} characters")
            print(f"  ğŸ”„ Loading converter module...")
            
            # Create a safe execution environment
            converter_globals = {}
            
            # Execute the converter code in isolated environment
            exec(converter_code, converter_globals)
            
            # Import the functions we need into the main namespace
            globals()['convert_notebook_to_kfp'] = converter_globals['convert_notebook_to_kfp']
            globals()['analyze_notebook_annotations'] = converter_globals['analyze_notebook_annotations']
            globals()['NotebookToKFPConverter'] = converter_globals['NotebookToKFPConverter']
            
            print(f"  âœ… Successfully loaded converter from: {path}")
            converter_loaded = True
            loaded_from = path
            break
            
        except Exception as e:
            print(f"  âŒ Error loading {path}: {e}")
            import traceback
            print(f"  ğŸ“‹ Traceback:")
            traceback.print_exc()
            continue
    else:
        print(f"  âŒ Not found: {path}")

if not converter_loaded:
    print("\\nâŒ CONVERTER LOADING FAILED")
    print("\\nğŸ“‹ Troubleshooting steps:")
    print("1. Ensure the converter file exists in one of these locations:")
    for path in converter_paths:
        print(f"   ğŸ“ {path}")
    print("2. Run the build process:")
    print("   ğŸ“¦ jlpm copy-python && jlpm build")
    print("3. Check file permissions and content")
    print("\\nğŸ“ Current directory structure:")
    try:
        current_dir = os.getcwd()
        print(f"   ğŸ“ Working directory: {current_dir}")
        
        for item in os.listdir('.'):
            if os.path.isdir(item) and item in ['src', 'lib']:
                print(f"   ğŸ“‚ {item}/")
                processors_dir = os.path.join(item, 'processors')
                if os.path.exists(processors_dir):
                    print(f"     ğŸ“‚ processors/")
                    for file in os.listdir(processors_dir):
                        if file.endswith('.py'):
                            size = os.path.getsize(os.path.join(processors_dir, file))
                            print(f"       ğŸ“„ {file} ({size} bytes)")
                else:
                    print(f"     âŒ processors/ directory not found")
            elif item.endswith('.py') and 'converter' in item:
                size = os.path.getsize(item)
                print(f"   ğŸ“„ {item} ({size} bytes)")
    except Exception as e:
        print(f"   âŒ Error listing directory: {e}")
    
    raise FileNotFoundError("Converter file not found in expected locations")

# Test the loaded converter functions
print("\\n=== Testing Converter Functions ===")
required_functions = ['convert_notebook_to_kfp', 'analyze_notebook_annotations', 'NotebookToKFPConverter']
missing_functions = []

for func_name in required_functions:
    if func_name in globals():
        print(f"âœ… {func_name}: Available")
        # Test if it's callable
        if callable(globals()[func_name]):
            print(f"   ğŸ”§ Function is callable")
        else:
            print(f"   âš ï¸ Object is not callable")
    else:
        print(f"âŒ {func_name}: Missing")
        missing_functions.append(func_name)

if missing_functions:
    print(f"\\nâš ï¸ Missing functions: {', '.join(missing_functions)}")
    print("ğŸ”§ The converter file may be incomplete or have syntax errors.")
else:
    print("\\nâœ… All required functions are available and ready!")

# Final status
print("\\n=== Initialization Status ===")
if converter_loaded and not missing_functions:
    print(f"ğŸ‰ Enhanced notebook to KFP converter ready!")
    print(f"ğŸ“ Loaded from: {loaded_from}")
    print(f"ğŸ”§ Functions available: {len(required_functions)}")
    result_message = "âœ… Enhanced converter initialized successfully - ready for notebook conversion!"
else:
    print("âš ï¸ Converter initialization completed with issues")
    result_message = "âš ï¸ Converter initialization completed with warnings - check logs above"

print("\\nğŸ’¡ Usage:")
print("   ğŸ“– analyze_notebook_annotations('notebook.ipynb')")
print("   ğŸ”„ convert_notebook_to_kfp('notebook.ipynb', 'output.py')")

result_message
    `;

    return this.executeCode(code);
  }

  /**
   * Enhanced notebook analysis with better error handling
   */
  async analyzeNotebookAnnotations(notebookPath: string): Promise<string> {
    const code = `
import os
import sys

print("=== Enhanced Notebook Annotation Analysis ===")

try:
    notebook_path = r"""${notebookPath}"""
    print(f"ğŸ” Analyzing: {notebook_path}")
    
    # Verify file exists
    if not os.path.exists(notebook_path):
        raise FileNotFoundError(f"âŒ Notebook not found: {notebook_path}")
    
    # Check file size
    file_size = os.path.getsize(notebook_path)
    print(f"ğŸ“Š File size: {file_size} bytes")
    
    if file_size == 0:
        raise ValueError("âŒ Notebook file is empty")
    
    # Check if converter functions are available
    if 'analyze_notebook_annotations' not in globals():
        raise NameError("âŒ analyze_notebook_annotations function not available. Please initialize the converter first.")
    
    print("ğŸ”„ Running analysis...")
    
    # Perform analysis with error handling
    try:
        analysis = analyze_notebook_annotations(notebook_path)
    except Exception as analysis_error:
        print(f"âŒ Analysis function failed: {analysis_error}")
        raise analysis_error
    
    if 'error' in analysis:
        print(f"âŒ Analysis failed: {analysis['error']}")
        raise Exception(analysis['error'])
    
    print("âœ… Analysis completed successfully!")
    print("\\nğŸ“Š Analysis Results:")
    print("=" * 50)
    print(f"ğŸ“– Total cells: {analysis['total_cells']}")
    print(f"ğŸ Code cells: {analysis['code_cells']}")
    print(f"ğŸ·ï¸ Annotated cells: {analysis['annotated_cells']}")
    print(f"âœ… Ready for conversion: {analysis['ready_for_conversion']}")
    
    if analysis['annotated_cells'] > 0:
        print(f"\\nğŸ“ Annotated Cell Details:")
        print("-" * 30)
        for i, cell_detail in enumerate(analysis['annotated_cell_details'], 1):
            cell_idx = cell_detail['cell_index']
            tags = cell_detail['tags']
            source_len = cell_detail.get('source_length', 0)
            print(f"  {i}. Cell {cell_idx + 1}:")
            print(f"     ğŸ·ï¸ Tags: {', '.join(tags)}")
            print(f"     ğŸ“ Code length: {source_len} characters")
        
        print(f"\\nğŸ¯ Pipeline Readiness:")
        if analysis['ready_for_conversion']:
            print("  âœ… Ready to convert to KFP pipeline!")
            print("  ğŸ’¡ Next step: Click 'Convert to KFP v2 DSL'")
        else:
            print("  âš ï¸ Not ready for conversion")
    else:
        print(f"\\nâš ï¸ No annotated cells found!")
        print("ğŸ’¡ To create a pipeline, add tags to your cells:")
        print("   ğŸ·ï¸ step:data-loading - for data ingestion")
        print("   ğŸ·ï¸ step:preprocessing - for data cleaning") 
        print("   ğŸ·ï¸ step:training - for model training")
        print("   ğŸ·ï¸ step:evaluation - for model evaluation")
        print("   ğŸ·ï¸ pipeline-parameters - for configuration")
        print("\\nğŸ“ How to add tags:")
        print("   1. Select a code cell")
        print("   2. Click a tag button in the Pipeline Builder panel")
        print("   3. Repeat for all relevant cells")
        print("   4. Run analysis again")
    
    print("=" * 50)
    
    f"ğŸ“Š Analysis complete: {analysis['annotated_cells']} annotated cells found ({'ready' if analysis['ready_for_conversion'] else 'not ready'} for conversion)"
    
except FileNotFoundError as e:
    error_msg = f"âŒ File error: {str(e)}"
    print(error_msg)
    raise e
except NameError as e:
    error_msg = f"âŒ Converter not initialized: {str(e)}"
    print(error_msg)
    print("ğŸ’¡ Please run 'Initialize Converter' first")
    raise e
except Exception as e:
    error_msg = f"âŒ Analysis failed: {str(e)}"
    print(error_msg)
    print("\\nğŸ”§ Troubleshooting:")
    print("  1. Ensure the notebook file is valid JSON")
    print("  2. Check that cells have proper metadata")
    print("  3. Verify file permissions")
    import traceback
    print("\\nğŸ“‹ Full traceback:")
    traceback.print_exc()
    raise e
    `;

    return this.executeCode(code);
  }

  /**
   * Enhanced notebook to KFP conversion with comprehensive error handling
   */
  async convertNotebookToKFP(notebookPath: string, outputPath?: string): Promise<string> {
    const code = `
import os
import sys
from datetime import datetime

print("=== Enhanced Notebook to KFP v2 DSL Conversion ===")

try:
    notebook_path = r"""${notebookPath}"""
    output_path = r"""${outputPath || notebookPath.replace('.ipynb', '_enhanced_kfp_pipeline.py')}"""
    
    print(f"ğŸ“– Input notebook: {notebook_path}")
    print(f"ğŸ“„ Output KFP DSL: {output_path}")
    print(f"â° Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Verify input file
    if not os.path.exists(notebook_path):
        raise FileNotFoundError(f"âŒ Notebook not found: {notebook_path}")
    
    file_size = os.path.getsize(notebook_path)
    print(f"ğŸ“Š Input file size: {file_size} bytes")
    
    # Check converter availability
    if 'convert_notebook_to_kfp' not in globals():
        raise NameError("âŒ convert_notebook_to_kfp function not available. Please initialize the converter first.")
    
    if 'analyze_notebook_annotations' not in globals():
        raise NameError("âŒ analyze_notebook_annotations function not available. Please initialize the converter first.")
    
    # Pre-conversion analysis
    print("\\nğŸ” Step 1: Pre-conversion Analysis")
    print("-" * 40)
    
    try:
        analysis = analyze_notebook_annotations(notebook_path)
    except Exception as analysis_error:
        print(f"âŒ Pre-analysis failed: {analysis_error}")
        raise analysis_error
    
    if 'error' in analysis:
        raise Exception(f"Pre-analysis failed: {analysis['error']}")
    
    print(f"ğŸ“Š Found {analysis['annotated_cells']} annotated cells")
    print(f"ğŸ“– Total cells: {analysis['total_cells']}")
    print(f"ğŸ Code cells: {analysis['code_cells']}")
    
    if not analysis['ready_for_conversion']:
        raise ValueError(f"âŒ No annotated cells found! Found {analysis['annotated_cells']} annotated cells. Please add tags like 'step:data-loading' to cells.")
    
    print(f"âœ… Pre-analysis passed - ready for conversion")
    
    # Conversion process
    print("\\nğŸ”„ Step 2: KFP v2 DSL Conversion")
    print("-" * 40)
    
    try:
        print("ğŸ”§ Converting notebook to KFP components...")
        kfp_code = convert_notebook_to_kfp(notebook_path, output_path)
        print(f"âœ… Conversion completed successfully!")
    except Exception as conversion_error:
        print(f"âŒ Conversion failed: {conversion_error}")
        import traceback
        print("ğŸ“‹ Conversion traceback:")
        traceback.print_exc()
        raise conversion_error
    
    # Verify output
    print("\\nğŸ” Step 3: Output Verification")
    print("-" * 40)
    
    if os.path.exists(output_path):
        output_size = os.path.getsize(output_path)
        print(f"âœ… Output file created: {output_size} bytes")
        
        # Quick validation of generated code
        try:
            with open(output_path, 'r', encoding='utf-8') as f:
                generated_code = f.read()
            
            # Basic checks
            if 'from kfp import dsl' in generated_code:
                print("âœ… KFP imports found")
            else:
                print("âš ï¸ KFP imports not found")
                
            if '@dsl.component' in generated_code:
                component_count = generated_code.count('@dsl.component')
                print(f"âœ… Found {component_count} KFP components")
            else:
                print("âš ï¸ No KFP components found")
                
            if '@dsl.pipeline' in generated_code:
                print("âœ… Pipeline function found")
            else:
                print("âš ï¸ Pipeline function not found")
                
        except Exception as validation_error:
            print(f"âš ï¸ Output validation failed: {validation_error}")
    else:
        print(f"âŒ Output file not created: {output_path}")
    
    # Success summary
    print("\\n" + "=" * 60)
    print("ğŸ‰ ENHANCED CONVERSION COMPLETED SUCCESSFULLY!")
    print("=" * 60)
    print(f"ğŸ“– Source: {os.path.basename(notebook_path)}")
    print(f"ğŸ“„ Generated: {os.path.basename(output_path)}")
    print(f"ğŸ“Š Code size: {len(kfp_code):,} characters")
    print(f"ğŸ”§ Components: {analysis['annotated_cells']}")
    print(f"â° Completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    print("\\nğŸ’¡ Next Steps:")
    print(f"   1. ğŸ“„ Review generated file: {output_path}")
    print(f"   2. ğŸ”¨ Compile pipeline: python {os.path.basename(output_path)}")
    print(f"   3. ğŸ§ª Test locally: python {os.path.basename(output_path)} --run-local")
    print(f"   4. ğŸš€ Deploy to KFP: python {os.path.basename(output_path)} --kfp-host <URL>")
    
    print("=" * 60)
    
    f"ğŸ‰ Enhanced conversion successful! Generated: {output_path}"
    
except FileNotFoundError as e:
    error_msg = f"âŒ File not found: {str(e)}"
    print(error_msg)
    print("ğŸ”§ Please ensure the notebook file exists and is accessible")
    raise e
except NameError as e:
    error_msg = f"âŒ Converter not ready: {str(e)}"
    print(error_msg)
    print("ğŸ’¡ Please initialize the converter first using 'Initialize Converter'")
    raise e
except ValueError as e:
    error_msg = f"âŒ Validation error: {str(e)}"
    print(error_msg)
    print("ğŸ’¡ Please add tags to your notebook cells:")
    print("   ğŸ·ï¸ Select cells and use the tagging buttons in the panel")
    raise e
except Exception as e:
    error_msg = f"âŒ Conversion failed: {str(e)}"
    print(error_msg)
    print("\\nğŸ”§ Troubleshooting steps:")
    print("  1. Check notebook format and structure")
    print("  2. Verify cell tags are properly formatted") 
    print("  3. Ensure cells contain valid Python code")
    print("  4. Try re-initializing the converter")
    import traceback
    print("\\nğŸ“‹ Full error traceback:")
    traceback.print_exc()
    raise e
    `;

    return this.executeCode(code);
  }

  /**
   * Install Python packages in the kernel
   */
  async installPackage(packageName: string): Promise<string> {
    const code = `
import subprocess
import sys

print(f"ğŸ“¦ Installing {packageName}...")

try:
    result = subprocess.run([sys.executable, '-m', 'pip', 'install', '${packageName}'], 
                          capture_output=True, text=True, check=True)
    print(f"âœ… Successfully installed ${packageName}")
    if result.stdout:
        print("ğŸ“‹ Installation output:")
        print(result.stdout)
    "âœ… Installation completed successfully"
except subprocess.CalledProcessError as e:
    error_msg = f"âŒ Failed to install ${packageName}"
    print(error_msg)
    if e.stderr:
        print(f"ğŸ“‹ Error details: {e.stderr}")
    raise e
    `;

    return this.executeCode(code);
  }

  /**
   * Get comprehensive kernel status and diagnostic information
   */
  async getKernelStatus(): Promise<string> {
    const code = `
import sys
import os
from datetime import datetime

print("=== Enhanced Kernel Status Report ===")
print(f"â° Report generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

# Python environment
print("\\nğŸ Python Environment:")
print(f"   Version: {sys.version}")
print(f"   Executable: {sys.executable}")
print(f"   Platform: {sys.platform}")
print(f"   Working directory: {os.getcwd()}")

# Memory usage
try:
    import psutil
    process = psutil.Process()
    memory_info = process.memory_info()
    print(f"   Memory usage: {memory_info.rss / 1024 / 1024:.1f} MB")
except ImportError:
    print("   Memory info: psutil not available")

# Check key packages
print("\\nğŸ“¦ Key Packages:")
key_packages = ['nbformat', 'kfp', 'pandas', 'numpy', 'sklearn', 'matplotlib']
for pkg in key_packages:
    try:
        module = __import__(pkg)
        if hasattr(module, '__version__'):
            print(f"   âœ… {pkg} v{module.__version__}")
        else:
            print(f"   âœ… {pkg} (version unknown)")
    except ImportError:
        print(f"   âŒ {pkg} not installed")

# Available converter functions
print("\\nğŸ”§ Converter Functions:")
converter_functions = ['convert_notebook_to_kfp', 'analyze_notebook_annotations', 'NotebookToKFPConverter']
available_count = 0
for func in converter_functions:
    if func in globals():
        print(f"   âœ… {func}")
        available_count += 1
    else:
        print(f"   âŒ {func}")

print(f"\\nğŸ“Š Converter status: {available_count}/{len(converter_functions)} functions available")

# Check for converter files
print("\\nğŸ“ Converter Files:")
converter_paths = [
    'src/processors/notebook_to_kfp_converter.py',
    'lib/processors/notebook_to_kfp_converter.py',
    'notebook_to_kfp_converter.py'
]

files_found = 0
for path in converter_paths:
    if os.path.exists(path):
        size = os.path.getsize(path)
        print(f"   âœ… {path} ({size:,} bytes)")
        files_found += 1
    else:
        print(f"   âŒ {path}")

# System capabilities
print("\\nğŸ”§ System Capabilities:")
print(f"   ğŸ“ File system access: {'âœ…' if os.access('.', os.R_OK | os.W_OK) else 'âŒ'}")
print(f"   ğŸŒ Network access: {'âœ…' if 'requests' in [pkg for pkg in key_packages if pkg in globals()] else 'â“'}")
print(f"   ğŸ’¾ Can write files: {'âœ…' if os.access('.', os.W_OK) else 'âŒ'}")

# Overall status
print(f"\\n=== Overall Status ===")
if available_count == len(converter_functions) and files_found > 0:
    status = "ğŸŸ¢ READY"
    print(f"{status} - All systems operational!")
    print("ğŸ’¡ Ready to analyze notebooks and generate KFP pipelines")
elif available_count > 0:
    status = "ğŸŸ¡ PARTIAL"
    print(f"{status} - Some functions available")
    print("ğŸ’¡ May need to re-initialize converter")
else:
    status = "ğŸ”´ NOT READY" 
    print(f"{status} - Converter not initialized")
    print("ğŸ’¡ Please run 'Initialize Converter' first")

f"ğŸ“Š Status: {status} | Functions: {available_count}/{len(converter_functions)} | Files: {files_found}/{len(converter_paths)}"
    `;

    return this.executeCode(code);
  }

  /**
   * Clean up kernel resources
   */
  async cleanup(): Promise<string> {
    const code = `
import gc
import sys

print("ğŸ§¹ Enhanced Kernel Cleanup...")

# Clear large variables
cleared_count = 0
memory_freed = 0

for name in list(globals().keys()):
    if not name.startswith('_') and name not in ['cleanup', 'gc', 'sys']:
        try:
            obj = globals()[name]
            if hasattr(obj, '__sizeof__'):
                obj_size = obj.__sizeof__()
                if obj_size > 1024*1024:  # > 1MB
                    memory_freed += obj_size
                    del globals()[name]
                    cleared_count += 1
        except:
            pass

# Force garbage collection
collected = gc.collect()

print(f"âœ… Cleanup completed:")
print(f"   ğŸ—‘ï¸ Cleared {cleared_count} large variables")
print(f"   ğŸ’¾ Freed ~{memory_freed // (1024*1024)} MB memory")
print(f"   ğŸ”„ Garbage collected: {collected} objects")

"ğŸ§¹ Enhanced cleanup completed successfully"
    `;

    return this.executeCode(code);
  }

  /**
   * Test converter functionality
   */
  async testConverter(): Promise<string> {
    const code = `
print("ğŸ§ª Testing Converter Functionality...")

try:
    # Test if functions are available
    functions_to_test = ['analyze_notebook_annotations', 'convert_notebook_to_kfp', 'NotebookToKFPConverter']
    
    print("\\nğŸ” Function Availability Test:")
    for func_name in functions_to_test:
        if func_name in globals():
            func = globals()[func_name]
            if callable(func):
                print(f"   âœ… {func_name}: Available and callable")
            else:
                print(f"   âš ï¸ {func_name}: Available but not callable")
        else:
            print(f"   âŒ {func_name}: Not available")
    
    # Test class instantiation
    print("\\nğŸ—ï¸ Class Instantiation Test:")
    if 'NotebookToKFPConverter' in globals():
        try:
            converter = NotebookToKFPConverter()
            print("   âœ… NotebookToKFPConverter: Can instantiate")
            
            # Test basic methods
            if hasattr(converter, 'convert_notebook'):
                print("   âœ… convert_notebook method: Available")
            else:
                print("   âŒ convert_notebook method: Missing")
                
            if hasattr(converter, '_is_pipeline_tag'):
                # Test tag recognition
                test_tags = ['step:data-loading', 'step:training', 'pipeline-parameters', 'invalid-tag']
                print("   ğŸ·ï¸ Tag Recognition Test:")
                for tag in test_tags:
                    is_valid = converter._is_pipeline_tag(tag)
                    status = "âœ…" if is_valid else "âŒ"
                    print(f"     {status} '{tag}': {is_valid}")
            
        except Exception as e:
            print(f"   âŒ NotebookToKFPConverter: Instantiation failed - {e}")
    else:
        print("   âŒ NotebookToKFPConverter: Class not available")
    
    # Test imports within converter
    print("\\nğŸ“¦ Import Dependencies Test:")
    required_imports = ['nbformat', 'os', 're', 'ast', 'json']
    for module_name in required_imports:
        try:
            __import__(module_name)
            print(f"   âœ… {module_name}: Available")
        except ImportError:
            print(f"   âŒ {module_name}: Missing")
    
    print("\\nğŸ‰ Converter testing completed!")
    
    "âœ… Converter functionality test completed - check results above"
    
except Exception as e:
    error_msg = f"âŒ Converter test failed: {e}"
    print(error_msg)
    import traceback
    traceback.print_exc()
    raise e
    `;

    return this.executeCode(code);
  }

  /**
   * Create a simple test notebook for demonstration
   */
  async createTestNotebook(filename: string = 'test_pipeline.ipynb'): Promise<string> {
    const code = `
import nbformat as nbf
import json

print(f"ğŸ“ Creating test notebook: {filename}")

# Create a new notebook
nb = nbf.v4.new_notebook()

# Add cells with pipeline annotations
cells = [
    # Markdown introduction
    nbf.v4.new_markdown_cell("# Test ML Pipeline\\n\\nThis is a test notebook for the Pipeline Builder extension."),
    
    # Data loading cell
    nbf.v4.new_code_cell(
        "# Data Loading\\nimport pandas as pd\\nimport numpy as np\\nfrom sklearn.datasets import load_iris\\n\\n# Load sample data\\niris = load_iris()\\ndf = pd.DataFrame(iris.data, columns=iris.feature_names)\\ndf['target'] = iris.target\\n\\nprint(f'Dataset shape: {df.shape}')\\nprint(df.head())",
        metadata={"tags": ["step:data-loading"]}
    ),
    
    # Preprocessing cell  
    nbf.v4.new_code_cell(
        "# Data Preprocessing\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# Split data\\nX = df.drop('target', axis=1)\\ny = df['target']\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Scale features\\nscaler = StandardScaler()\\nX_train_scaled = scaler.fit_transform(X_train)\\nX_test_scaled = scaler.transform(X_test)\\n\\nprint(f'Training set shape: {X_train_scaled.shape}')\\nprint(f'Test set shape: {X_test_scaled.shape}')",
        metadata={"tags": ["step:preprocessing"]}
    ),
    
    # Training cell
    nbf.v4.new_code_cell(
        "# Model Training\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.metrics import accuracy_score, classification_report\\n\\n# Train model\\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\\nmodel.fit(X_train_scaled, y_train)\\n\\n# Make predictions\\ny_pred = model.predict(X_test_scaled)\\n\\nprint('Model training completed!')\\nprint(f'Model type: {type(model).__name__}')",
        metadata={"tags": ["step:training"]}
    ),
    
    # Evaluation cell
    nbf.v4.new_code_cell(
        "# Model Evaluation\\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\\n\\n# Calculate metrics\\naccuracy = accuracy_score(y_test, y_pred)\\nprecision = precision_score(y_test, y_pred, average='weighted')\\nrecall = recall_score(y_test, y_pred, average='weighted')\\nf1 = f1_score(y_test, y_pred, average='weighted')\\n\\nprint(f'Accuracy: {accuracy:.4f}')\\nprint(f'Precision: {precision:.4f}')\\nprint(f'Recall: {recall:.4f}')\\nprint(f'F1-score: {f1:.4f}')\\n\\nmetrics = {\\n    'accuracy': accuracy,\\n    'precision': precision,\\n    'recall': recall,\\n    'f1_score': f1\\n}",
        metadata={"tags": ["step:evaluation"]}
    ),
    
    # Parameters cell
    nbf.v4.new_code_cell(
        "# Pipeline Parameters\\n\\n# Model parameters\\nn_estimators = 100\\nrandom_state = 42\\ntest_size = 0.2\\n\\n# Data parameters\\ntarget_column = 'target'\\nfeature_columns = ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\\n\\nprint('Pipeline parameters defined')\\nprint(f'n_estimators: {n_estimators}')\\nprint(f'test_size: {test_size}')",
        metadata={"tags": ["pipeline-parameters"]}
    )
]

# Add cells to notebook
nb.cells = cells

# Add notebook metadata
nb.metadata = {
    "kernelspec": {
        "display_name": "Python 3",
        "language": "python", 
        "name": "python3"
    },
    "language_info": {
        "name": "python",
        "version": "3.8.0"
    },
    "pipeline": {
        "name": "iris_classification",
        "description": "Simple iris classification pipeline for testing"
    }
}

# Write notebook file
try:
    with open('${filename}', 'w', encoding='utf-8') as f:
        nbf.write(nb, f)
    
    print(f"âœ… Test notebook created successfully!")
    print(f"ğŸ“„ File: ${filename}")
    print(f"ğŸ“Š Cells: {len(nb.cells)} total")
    print(f"ğŸ·ï¸ Tagged cells: {len([cell for cell in nb.cells if cell.get('metadata', {}).get('tags')])}")
    
    # Verify file was created
    import os
    if os.path.exists('${filename}'):
        file_size = os.path.getsize('${filename}')
        print(f"ğŸ“ File size: {file_size:,} bytes")
    
    print("\\nğŸ’¡ Usage:")
    print(f"   1. Open {filename} in JupyterLab")
    print(f"   2. Use Pipeline Builder to analyze and convert")
    print(f"   3. All cells are already tagged for you!")
    
    f"âœ… Test notebook '{filename}' created with {len([cell for cell in nb.cells if cell.get('metadata', {}).get('tags')])} tagged cells"
    
except Exception as e:
    error_msg = f"âŒ Failed to create test notebook: {e}"
    print(error_msg)
    raise e
    `;

    return this.executeCode(code);
  }
}