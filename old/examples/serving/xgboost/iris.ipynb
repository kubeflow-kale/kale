{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Install necessary packages\n",
    "\n",
    "We can install the necessary packages by either running `pip install --user <package_name>` or include everything in a `requirements.txt` file and run `pip install --user -r requirements.txt`.\n",
    "\n",
    "> NOTE: Do not forget to use the `--user` argument. It is necessary if you want to use Kale to transform this notebook into a Kubeflow pipeline"
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip3 install --user -r requirements.txt"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imports\n",
    "\n",
    "In this section we import the packages we need for this example. Make it a habbit to gather your imports in a single place. It will make your life easier if you are going to transform this notebook into a Kubeflow pipeline using Kale."
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "tags": [
     "imports"
    ]
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Project hyper-parameters\n",
    "\n",
    "In this cell, we define the different hyper-parameters variables. Defining them in one place makes it easier to experiment with their values and also facilitates the execution of HP Tuning experiments using Kale and Katib."
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "ETA = .3\n",
    "MAX_DEPTH = 3\n",
    "OBJECTIVE = \"multi:softprob\"\n",
    "STEPS = 20"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "tags": [
     "pipeline-parameters"
    ]
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load and preprocess data\n",
    "\n",
    "In this section, we load and process the dataset to get it in a ready-to-use form by the model."
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "x, y = datasets.load_iris(return_X_y=True)"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:load_transform_data"
    ]
   }
  },
  {
   "cell_type": "code",
   "source": [
    "x_trn, x_tst, y_trn, y_tst = train_test_split(x, y, test_size=.2)"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "D_trn = xgb.DMatrix(x_trn, label=y_trn)\n",
    "D_tst = xgb.DMatrix(x_tst, label=y_tst)"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define and train the model\n",
    "\n",
    "We are now ready to define our model. In this example, we use the Extreme Gradient Boosting algorithm inmplemented by [XGBoost](https://xgboost.ai/)."
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "param = {\"eta\": float(ETA),\n",
    "         \"max_depth\": int(MAX_DEPTH),\n",
    "         \"objective\": OBJECTIVE,\n",
    "         \"num_class\": 3}\n",
    "\n",
    "steps = int(STEPS)"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:train_model",
     "prev:load_transform_data"
    ]
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model = xgb.train(param, D_trn, steps)"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluate the model\n",
    "\n",
    "Finally, we are ready to evaluate the model using the test set."
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "preds = model.predict(D_tst)\n",
    "max_preds = np.asarray([np.argmax(line) for line in preds])"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:evaluate_model",
     "prev:load_transform_data",
     "prev:train_model"
    ]
   }
  },
  {
   "cell_type": "code",
   "source": [
    "precision = precision_score(y_tst, max_preds, average='macro')\n",
    "recall = recall_score(y_tst, max_preds, average='macro')\n",
    "f1 = f1_score(y_tst, max_preds, average='macro')\n",
    "accuracy = accuracy_score(y_tst, max_preds)"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Serving\n",
    "\n",
    "We can deploy the model as an inference server to KFServing, using the Kale `serve` API."
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from kale.common.serveutils import serve\n",
    "kfserver = serve(model)"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "data = {\"instances\": [[6.8, 2.8, 4.8, 1.4], [5.1, 3.5, 1.4, 0.2]]}\n",
    "res = kfserver.predict(json.dumps(data))\n"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(res)"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pipeline metrics\n",
    "\n",
    "In the last cell of the Notebook, we print the pipeline metrics. These will be picked up by Kubeflow Pipelines, which will make them available through its UI."
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(precision)\n",
    "print(recall)\n",
    "print(f1)\n",
    "print(accuracy)"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "tags": [
     "pipeline-metrics"
    ]
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kubeflow_notebook": {
   "katib_run": true,
   "pipeline_name": "iris-pipeline",
   "steps_defaults": [
    "label:access-ml-pipeline:true",
    "label:access-rok:true"
   ],
   "experiment_name": "iris-experiment",
   "autosnapshot": true,
   "katib_metadata": {
    "algorithm": {
     "algorithmName": "grid",
     "algorithmSettings": [
      {
       "name": "random_state",
       "value": "10"
      },
      {
       "name": "acq_optimizer",
       "value": "auto"
      },
      {
       "name": "acq_func",
       "value": "gp_hedge"
      },
      {
       "name": "base_estimator",
       "value": "GP"
      }
     ]
    },
    "maxFailedTrialCount": 3,
    "maxTrialCount": 15,
    "objective": {
     "additionalMetricNames": [
      "precision",
      "recall",
      "f1"
     ],
     "goal": 1,
     "objectiveMetricName": "accuracy",
     "type": "maximize"
    },
    "parallelTrialCount": 3,
    "parameters": [
     {
      "feasibleSpace": {
       "max": "0.3",
       "min": "0.1",
       "step": "0.1"
      },
      "name": "ETA",
      "parameterType": "double"
     },
     {
      "feasibleSpace": {
       "max": "6",
       "min": "2",
       "step": "1"
      },
      "name": "MAX_DEPTH",
      "parameterType": "int"
     },
     {
      "feasibleSpace": {
       "list": [
        "multi:softprob",
        "multi:softmax"
       ]
      },
      "name": "OBJECTIVE",
      "parameterType": "categorical"
     },
     {
      "feasibleSpace": {
       "max": "100",
       "min": "20",
       "step": "10"
      },
      "name": "STEPS",
      "parameterType": "int"
     }
    ]
   },
   "volumes": [
    {
     "annotations": [],
     "mount_point": "/home/jovyan",
     "name": "workspace-examples-serving-bgsd4h5em",
     "size": 5,
     "size_type": "Gi",
     "snapshot": false,
     "type": "clone"
    },
    {
     "annotations": [],
     "mount_point": "/home/jovyan/data",
     "name": "data-8lbdjmzdh",
     "size": 10,
     "size_type": "Gi",
     "snapshot": false,
     "type": "clone"
    }
   ],
   "docker_image": "dimpo/jupyter-kale-cpu:v0.5.1-xgboost-v0.0.1",
   "experiment": {
    "id": "new",
    "name": "iris-experiment"
   },
   "snapshot_volumes": true,
   "pipeline_description": "Train an XGBoost model on the Iris dataset"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "0.25.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
